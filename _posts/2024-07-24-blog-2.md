---
title: "From Control Hamiltonian to Algebraic Riccati Equation and Pontryagin's Maximum Principle"
date: 2024-07-16
permalink: /posts/2024/07/16
tags:
  - Algebraic Riccati Equation
  - Linear Qudratic Regulator
  - Calculus of Variations
---
Inspired by the Hamiltonian of classical mechanics, Lev Pontryagin introduced the [**_Control Hamiltonian_**](https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)) and formulated his celebrated [**_Pontragin's Maximum Principle (PMP)_**](https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle). This blog will first discuss general cases of optimal control problems, including scenarios with both free final time and final states. Then, the derivation of [**_Algebraic Riccati Equation (ARE)_**](https://en.wikipedia.org/wiki/Algebraic_Riccati_equation) in the context of **_Continuous Linear Qudratic Regulator (LQR)_** from the perspective of the **_Control Hamiltonian_** will be introduced. It will then explain the **_PMP_**, finally followed by the example problem of **_Constrained Continous LQR_**.

## More general senarios of trajectory optimization problems

In [one of the previous blogs](https://lihanlian.github.io/posts/2024/06/30) on Euler-Lagrange equation, I went through the derivation process for the case of fixed final state and free final time. Now, Let's consider more general cases that involve free final state and free final time. To start with, recall the goal is minimize the objective functional as follows:

$$ J[x] = \int_0^{t_f} L(x(t), \dot{x}(t)) \, dt $$

The first varition of \\(J[x]\\) can be written as:

$$ \delta J[x] = \int_0^{t_f} \left( \frac{\partial L}{\partial x} \delta x + \frac{\partial L}{\partial \dot{x}} \delta \dot{x} \right) dt $$

For the optimal solution, the first varition of the functional \\(J[x]\\) should be zero. Note that in the later section, for the sake of concise notation, I will use $$ L^* \big|_{t_f} $$ to denote \\(L^*(x(t_f), \dot{x}(t_f))\\). 
Also keep in mind that I will denote \\(x(t)\\), \\(\dot{x}(t)\\)  as  $$x$$, $$\dot{x}$$ for simplicity too. 

Now consider the most general case, where both final state and final time are free, the first variation can be expressed as:

$$ \delta J[x^*] \approx \int_0^{t_f} \left( \frac{\partial L^*}{\partial x} \delta x + \frac{\partial L^*}{\partial \dot{x}^*} \delta \dot{x} \right) \delta x \, dt + \int_{t_f}^{t_f + \delta t_f} L^* \big|_{t_f} \, dt$$

<figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
  <img src='/images/blog/blog2-fig1.png' style="width: 100%;">
  <figcaption style="text-align: center;">Graphical illustration of different components in the equations</figcaption>
</figure>

where $$\int_{t_f}^{t_f + \delta t_f} L^* \big|_{t_f} \, dt$$ comes from the fact that the final time $$\delta t_f$$ is no longer fixed. 
The reason we can approximate the increment of the objective functional caused by the change of final time as this is that we are doing small perturbation at the optimal $$J[x^*]$$. Through integration by part and the fact of fixed initial condition (as shown in the figure above), we can further rearrange the equation as follows:

$$ \delta J[x^*] = \int_0^{t_f} \left( \frac{\partial L^*}{\partial x^*} - \frac{d}{dt} \frac{\partial L^*}{\partial \dot{x}^*} \right) \delta x \, dt + \int_{t_f}^{t_f + \delta t_f} L^* \big|_{t_f} \, dt + \frac{\partial L^*}{\partial \dot{x}^*} \delta x(t_f) $$

Again, since the perturbation is small, the resulting change of time interval \\(\delta t_f\\) is also small. Further approxmation can be made:

$$ \int_{t_f}^{t_f + \delta t_f} L^* \big|_{t_f} \, dt = L^* \big|_{t_f} \delta t_f$$ 

$$ \delta x_f \approx \delta x(t_f) + \dot{x}^* \delta t_f $$

Thus, $$ \delta J[x^*]$$ can be rewritten as:

$$ = \int_0^{t_f} \left( \frac{\partial L^*}{\partial x^*} - \frac{d}{dt} \frac{\partial L^*}{\partial \dot{x}^*} \right) \delta x^* \, dt + L^* \big|_{t_f} \delta t_f + \frac{\partial L^*}{\partial \dot{x}^*} \left( \delta x_f - \dot{x}^* \delta t_f \right) $$

$$ = \int_0^{t_f} \left( \frac{\partial L^*}{\partial x^*} - \frac{d}{dt} \frac{\partial L^*}{\partial \dot{x}} \right) \delta x^* \, dt + \frac{\partial L^*}{\partial \dot{x}^*} \delta x_f + \left( L^* \big|_{t_f} - \frac{\partial L^*}{\partial \dot{x}^*} \dot{x}^*(t_f) \right) \delta t_f $$

 - Euler-Lagrange Equation

$$\frac{\partial L^*}{\partial x^*} - \frac{d}{dt} \frac{\partial L^*}{\partial \dot{x}^*} = 0$$

Since there is no constraint and $$\delta x$$ can be arbirtray, the only way to make the term including $$\delta x$$ zero is set the Euler-Lagragne Equation to zero and this needs to be held at all times. In addition, since this is a **$$2^{\text{nd}}$$ order differential equations**, we need $$2n$$ boundary conditions to solve it(for $$x \in R^n$$). We have $$n$$ boundary conditions from the given $$x_0$$. For other cases:

- **Fixed final state with fixed final time**

The fixed final state value of $$x(t_f)$$ provide the other $$n$$ boundary conditions. Both $$\delta x_f$$ and $$\delta t_f$$ are zero.

- **Fixed final state with free final time**

The fixed final state value of $$x(t_f)$$ provide the other $$n$$ boundary conditions. $$\delta x_f$$ is zero. Since $$\delta t_f$$ is no longer zero, and the case of free final time introduce a new variable ($$t_f$$), so we need to solve the following (a scalar equation) to get final time $$t_f$$.

$$ L^* \big|_{t_f} - \frac{\partial L^*}{\partial \dot{x}^*} \dot{x}^*(t_f) = 0$$

- **Free final state with fixed final time**

$$\delta x_f$$ is no longer zero, so we need to solve the following to get the other $$n$$ boundary conditions. $$\delta t_f$$ is zero.

$$\frac{\partial L^*}{\partial \dot{x}^*}  = 0$$

- **Free final state with free final time**

Both $$\delta x_f$$ and $$\delta t_f$$ are non-zero. Both following equations need to be solved to get enough boundary conditions:

$$ L^* \big|_{t_f} - \frac{\partial L^*}{\partial \dot{x}^*} \dot{x}^*(t_f) = 0$$

$$\frac{\partial L^*}{\partial \dot{x}^*}  = 0$$

## Example 1: Solving a simple trajectory optimization problem

Consider the following simple robot dynamics (single-integrator model) and the objetive functional aimed to minimize:

$$ J = \frac{1}{2} \int_{0}^{1} (x^2 + u^2) \, dt $$

$$ s.t. \dot{x} = u(t)$$

This is also in [one of the previous blogs](https://lihanlian.github.io/posts/2024/06/30) on Euler-Lagrange Equation (second example), but is a fixed final state and fixed final time problem (\\(x(0) = 0, x(1) = 10,\\)). Now, Let's take a look at more general cases including free final state and/or free final time. 



## Introducing the control input and terminal cost

Now we introduce the control input $$u(t)$$ and the terminal cost function $$\Phi(x(t_f), t_f)$$ and Let's derive the expression for the most general case that allow both free final state and free final time. First of all, the optimization problme is formulated as follows:

$$J[x,u] = \int_0^{t_f} L(x(t), u(t)) \, dt + \Phi(x(t_f), t_f) \quad \text{s.t.} \quad \dot{x}(t) = f(x(t), u(t))$$

To handle the equality constraints due to system dynamcis, we introduce the lagrange multiplier $$\lambda$$ (which is also a function of time) and reformulate the objective functional:

$$
J[x,u,\lambda] = \int_0^{t_f} \left[ L(x(t), u(t)) + \lambda^T(t) (f(x(t), u(t)) - \dot{x}(t)) \right] dt + \Phi(x(t_f), t_f)
$$

Define the Lagrangian $$\mathcal{L} = L(x,u) + \lambda^T (f(x,u) - \dot{x}(t))$$ and the first variation is:

$$
\delta J^*[x,u,\lambda] = \int_0^{t_f} \left[ \left( \nabla_x \mathcal{L}^* - \frac{d}{dt} \nabla_{\dot{x}} \mathcal{L}^* \right) \delta x + \nabla_u \mathcal{L}^* \delta u \right] dt $$

$$
+ \nabla_{\dot{x}} \mathcal{L}^*\big|_{t_f} \delta x_{f} + \left[\mathcal{L}^*\big|_{t_f} - \nabla_x \mathcal{L}^*\big|_{t_f}\dot{x}^*(t_f)\right] \delta t_f
$$

$$
+ \nabla_x \Phi^* \big|_{t_f} \delta x_{f} + \frac{\partial \Phi^*\big|_{t_f}}{\partial t_f} \delta t_f
$$

where $$\nabla_{\dot{x}} \mathcal{L}^*\big|_{t_f} \delta x_{f}$$ comes from integration by part, similiar to the previous derivation of Euler-Lagrange Equation.Rearrange the term to separate $$\delta t, \delta x_{f}, \text{and } \delta t_{f}$$:
<!-- Rearrange the term to separate $$\delta t$$, $$\delta x_{f}$$ and $$\delta t_{f}$$: -->

$$
\delta J^*[x,u,\lambda] = \int_0^{t_f} \left[ \left( \nabla_x \mathcal{L}^* - \frac{d}{dt} \nabla_{\dot{x}} \mathcal{L}^* \right) \delta x + \nabla_u \mathcal{L}^* \delta u \right] dt $$

$$
+ \left[ \nabla_{\dot{x}} \mathcal{L}^*\big|_{t_f} + \nabla_x \Phi^* |_{t_f}\right]\delta x_{f} 
$$

$$
+ \left[ \mathcal{L}^*\big|_{t_f} - \nabla_x \mathcal{L}^*\big|_{t_f}\dot{x}^*(t_f) + \frac{\partial \Phi^*\big|_{t_f}}{\partial t_f}\right] \delta t_f 
$$

Thus, when encounters an optimal control problem, the general procedures can be think of as the following:

```python
def Brain(problem_setting):
  # Determine the type of optimal control problem
  return delta_xf, delta_tf

problem_solved = False
delta_xf, delta_tf = Brain(problem_setting) 
while problem_solved is not True:
  # For free final state case
  if delta_xf is not 0:
    # solve the following equations
```

$$
\nabla_{\dot{x}} \mathcal{L}^*\big|_{t_f} + \nabla_x \Phi^* |_{t_f} = 0 
$$

```python
  # For free final time case
  if delta_tf is not 0:
    # solve the following equations
```

$$ \nabla_{\dot{x}} \mathcal{L}^* - \nabla_x \mathcal{L}^*\big|_{t_f}\dot{x}^*(t_f) + \frac{\partial \Phi^*\big|_{t_f}}{\partial t_f} = 0$$

```python
  problem_solved = True # Hopefully
```

Introduce the **Control Hamiltonian** to make the expression more compact:

$$
\mathcal{H}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{\lambda}(t)) = g(\mathbf{x}(t), \mathbf{u}(t)) + \mathbf{\lambda}^T(t)[a(\mathbf{x}(t), \mathbf{u}(t))]
$$

$$\dot{x}^* = \nabla_{\lambda}\mathcal{H}^*(x^*(t), u^*(t), \lambda^*(t))$$

$$\dot{\lambda}^* = -\nabla_{x}\mathcal{H}^*(x^*(t), u^*(t), \lambda^*(t))$$

## Example 2: Continuous LQR and ARE

Consider the following final horizon and free final state LQR problem desribed as follows. We will show that **Algebraic Riccati Equation (ARE)** can be derived 
using the previously mentioned procedures as $$T \to \infty$$. 

$$ J = \frac{1}{2} \int_{0}^{T} (x(t)^TQx(t) + u(t)^TRu(t)) \, dt + \frac{1}{2} x(t)^T H x(t) $$
 
$$ s.t. \quad \dot{x} = Ax(t) + Bu(t) $$

where $$Q > 0$$, $$R \geq 0$$ and $$H$$ is a constant positive definite matrix.

 - Formulate Control Hamiltonian:

$$ \mathcal{H}(x, u, \lambda, t) = \frac{1}{2} (x^TQx + u^TRu) + \lambda^T (Ax + Bu) $$

 - Construsct state and co-state equations:

$$ \dot{x}^* = \nabla_{\lambda}{\mathcal{H}}(x, u, \lambda, t) = Ax + Bu $$

$$ \dot{\lambda}^* = -\nabla_{x}{\mathcal{H}}(x, u, \lambda, t) = -Qx - A^T \lambda $$

 - Obtaining Optimal \\( u \\):

Since we are __NOT__ concerned with the control input constraints here, the optimal control input \\(u\\) can be directly obtained by setting the following equation to zero.

$$ \nabla_u{\mathcal{H}} = Ru + B^T\lambda = 0, \quad  u^* = -R^{-1}B^T\lambda$$

Substitute $$u^*$$ into state equation and put them all together into matrix form:

$$
\begin{bmatrix}
\dot{\mathbf{x}}^*(t) \\
\dot{\mathbf{\lambda}}^*(t)
\end{bmatrix}
=
\begin{bmatrix}
A & -B R^{-1} B^T \\
-Q & -A^T
\end{bmatrix}
\begin{bmatrix}
\mathbf{x}^*(t) \\
\mathbf{\lambda}^*(t)
\end{bmatrix}
$$

Now suppose the costate $$\lambda$$ is linearly dependent on state $$x$$ <span style="color:red">(See Appendix for more detail about the validation of this assumption)</span>, and its time derivative can be expressed as follows:

$$
\mathbf{\lambda}^*(t) = \mathbf{P}(t) \mathbf{x}^*(t)
$$

$$
\dot{\mathbf{\lambda}}^*(t) =\dot{\mathbf{P}}(t) \mathbf{x}^*(t) + \mathbf{P}(t) \dot{\mathbf{x}}^*(t)
$$

where $$P(t)$$ is a **time varying matrix**. Substitute into state space representation:

$$
\begin{bmatrix}
\dot{\mathbf{x}}^* \\
\dot{P}\mathbf{x}^* + P\dot{\mathbf{x}}^*
\end{bmatrix}
=
\begin{bmatrix}
A & -B R^{-1} B^T \\
-Q & -A^T
\end{bmatrix}
\begin{bmatrix}
\mathbf{x}^*(t) \\
\mathbf{\lambda}^*(t)
\end{bmatrix}
$$

 - <span style="color:red">Riccati Differntial Equation (RDE) and Algebraic Riccati Equation (ARE)</span>:

From costate equation, we can obtain:

$$ \dot{\lambda}^* = \dot{P}x^* + P(Ax^* - BR^{-1}B^{T}Px^*) = -Qx^* - A^{T}Px^* $$

$$ \dot{P}x^* + PAx^* - PBR^{-1}B^{T}Px^* = -Qx^* - A^{T}Px^* $$

$$ \dot{P} + PA + A^{T}P - PBR^{-1}B^{T}P + Q = 0 \tag{1} \label{eq:RDE}$$

Now, we get the **RDE** \eqref{eq:RDE} that $$P(t)$$ evolves accroding to. For a stable system, when $$T$$ (upper limit of integral) approachs infinity, matrix $$P(t)$$ also converges to a constant value and $$\dot{P}(t)$$ becomes to zero <span style="color:red">(See Appendix for more detail about the validation of this assumption).</span> This give rises to the **ARE**:

$$ PA + A^{T}P - PBR^{-1}B^{T}P + Q = 0 \tag{2} \label{eq:ARE}$$

- Boundary conditions:

We have $$2n$$ first order differential equations ($$n$$ from the $$x$$ and $$n$$ from $$\lambda$$), so we need $$2n$$ boundary conditions to solve the problem. Given initial condition of $$x_0$$, we have $$n$$ boundary conditions at the initial time of state equation. 

Recall we are dealing with the type of free final state fixed final time problem. Equipped with the tool we just derived from previous section, the other $$n$$ boundary conditions for this type of problem can be obtained by setting the term in front of $$\delta x_f$$ to zero.

$$ \nabla_{\dot{x}} \mathcal{L}^*\big|_{t_f} + \nabla_x \Phi^* |_{t_f} = 0 $$

where 

$$\mathcal{L}^* = \frac{1}{2} (x^{*T}Qx^* + u^{*T}Ru^*) + \lambda^T (Ax^* + Bu^* - \dot{x}^*)$$

$$ \Phi^* |_{t_f} = \frac{1}{2} (x^{*T}(t_f)Hx^*(t_f))$$

$$ -\lambda^*(t_f) + Hx^*(t_f) =  0 , \quad \lambda^*(t_f) = Hx^*(t_f)$$

Thus, the other $$n$$ boundary conditions come from the costate state equation at the final time and <span style="color:red"> we need to solve the ARE **backward** in time </span> to get costate trajectory $$\lambda^*(t)$$. Then the optimal control input trajectory $$u^*(t)$$ can obtained by the preivously derived optmality condition ($$u^*(t) = -R^{-1}B^T\lambda^*(t)$$).

## Pontryagin's Maximum Principle

**- Continuous time PMP**
For a given system an a give optimal criterion, let $$u^* \in \mathcal{U}$$ be <span style="color:red">an</span> optimal control, then there is a variable called costate which together with the state variables satifsfies the Hamilton canonical equations. In addition, it turns out Pontryagin's Maximum Principle is also frequently refered as Pontryagin's Minimum Principle. The difference comes from the different formulasim of the **Control Hamiltonian**. For <span style="color:red">Pontryagin's **Maximum** Principle</span>, the **Control Hamiltonian (maximum)** is defined as follows:

$$ \mathcal{H}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{\lambda}(t), t) = -L(\mathbf{x}(t), \mathbf{u}(t)) + \mathbf{\lambda}^T(t)[f(\mathbf{x}(t), \mathbf{u}(t))] $$

$$\dot{x}^* = \nabla_{\lambda}\mathcal{H}(x^*(t), u^*(t), \lambda^*(t), t)$$

$$\dot{\lambda}^* = -\nabla_{x}\mathcal{H}(x^*(t), u^*(t), \lambda^*(t), t)$$

$$u^* = \arg \max_{u \in \mathcal{U}} H(x(t)^*, u(t), \lambda(t)^*, t) $$

For <span style="color:red">Pontryagin's **Minium** Principle</span>, the **Control Hamiltonian (minimum)** is defined as follows:

$$ \mathcal{H}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{\lambda}(t), t) = L(\mathbf{x}(t), \mathbf{u}(t)) + \mathbf{\lambda}^T(t)[f(\mathbf{x}(t), \mathbf{u}(t))] $$

This results in the optimal control is the one that minimize the **Control Hamiltonian**:

$$u^* = \arg \min_{u \in \mathcal{U}} H(x(t)^*, u(t), \lambda(t)^*, t) $$

**- Corollaries \[8\]** 

Pontryagin and his co-workers have also derived other necessary conditions for optimality that we will find useful. We now state, without proof, two of these necessary conditions:

1. If the final time is fixed and the Hamiltonian does not depend explicitly on time, then the Hamiltonian must be a constant when evaluated on an extremal trajectory; that is,

   $$\mathcal{H}(x^*(t), u^*(t), \lambda^*(t)) = c_1 \quad \text{for} \; t \in [t_0, t_f].$$

2. If the final time is free, and the Hamiltonian does not explicitly depend on time, then the Hamiltonian must be identically zero when evaluated on an extremal trajectory; that is,

   $$\mathcal{H}(x^*(t), u^*(t), \lambda^*(t)) = 0 \quad \text{for} \; t \in [t_0, t_f].$$


**- Discrete time PMP and KKT \[2\]** 

In discrete time setting, the PMP can be viewed as a special case of [Karush–Kuhn–Tucker conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) (KKT), check out the second reference for more details from Prof. Zac Manchester. Let's first take a look at the optimization problem formulated as follows:

$$ \min_{x_k, u_k} \sum_{k=0}^{N-1} L(x_k, u_k) + \Phi(x_N) $$

$$ s.t. \quad x_{k+1} = f(x_k, u_k) $$

Same as before, \\(f(x_k, u_k)\\) defines the system dynamics and \\( L(x_k, u_k), \Phi(x_N)\\) are stage cost and terminal cost function respectively. We can then formulate the Lagrangian and proceed the procedure in the KKT manner:

$$ \mathcal{L} = \sum_{k=0}^{N-1} \left[ L(x_k, u_k) + \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right) \right] + \Phi(x_N) \tag{3} \label{eq:Lagrangian}$$

$$ \frac{\partial \mathcal{L}(x_k, u_k)}{\partial \lambda_{k+1}} = f(x_k, u_k) - x_{k+1} = 0 \quad (k = 0, \cdots ,N-1) \tag{4} \label{eq:state}$$

$$ \frac{\partial \mathcal{L}}{\partial x_k} = \frac{\partial L(x_k, u_k)}{\partial x_k} + \left( \frac{\partial f(x_k, u_k)}{\partial x_k} \right)^\top \lambda_{k+1} - \lambda_k \\(k = 1, ..., N-1 \\) \tag{5} \label{eq:costate}$$

$$  \frac{\partial \mathcal{L}}{\partial x_N} = \frac{\partial \Phi(x_N)}{\partial x_N} - \lambda_N \quad \\(k = N \\) \tag{6} \label{eq:boundary}$$

To clarify the dimensions of each equations, suppose \\(x\\) has \\(n\\) elements, which is a (\\( n \times 1 \\)) vector at each time step \\(k\\) (and so is $$\lambda$$), and there are \\( k = N\\) time step. At each time step \\(k \\), the partial derivative of Lagrangian 

- w.r.t \\( \lambda_{k+1}\\) is a \\( n \times 1 \\) column vector. From \\(k = 0 \cdots N-1\\), results in \\(n \times N\\) equations. Note that we are also given \\(x_0\\), **which is a boundary condition at \\(k = 0 \\) with dimension \\(n \times 1\\)**.

$$
\frac{\partial  \mathcal{L}(x_k, u_k)}{\partial \lambda_{k+1}} = 
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial \lambda_{(k+1)1}} \\
\frac{\partial \mathcal{L}}{\partial \lambda_{(k+1)2}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial \lambda_{(k+1)n}}
\end{bmatrix}
$$

- w.r.t \\( x_k\\) is a \\( n \times 1 \\) column vector. From \\(k = 1, \cdots, N\\), results in \\(n \times (N-1)\\) euqations. 

$$
\frac{\partial  \mathcal{L}(x_k, u_k)}{\partial x_k} = 
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial x_{k1}} \\
\frac{\partial \mathcal{L}}{\partial x_{k2}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial x_{kn}}
\end{bmatrix}
$$

Since \\(x_k\\) is a \\(n \times 1\\) column vector, \\( f(x_k, u_k) \\) is a also a vector, so its Jacobian matrix (\\(n \times n\\))with respect to \\( x_k \\) is given by:

$$
\frac{\partial f(x_k, u_k)}{\partial x_k} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_{k1}} & \frac{\partial f_1}{\partial x_{k2}} & \cdots & \frac{\partial f_1}{\partial x_{kn}} \\
\frac{\partial f_2}{\partial x_{k1}} & \frac{\partial f_2}{\partial x_{k2}} & \cdots & \frac{\partial f_2}{\partial x_{kn}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_{k1}} & \frac{\partial f_n}{\partial x_{k2}} & \cdots & \frac{\partial f_n}{\partial x_{kn}}
\end{bmatrix}
$$

The term \\(-\lambda_k\\) appears because \\(x_k\\) is also present in term \\(\lambda_k^\top \left( f(x_{k-1}, u_{k-1}) - x_{k} \right)\\) from the previous time step \\(k-1\\), thus:

$$
\begin{bmatrix}
\frac{\partial \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right)}{\partial \lambda_{(k+1)1}} \\
\frac{\partial \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right)}{\partial \lambda_{(k+1)2}} \\
\vdots \\
\frac{\partial \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right)}{\partial \lambda_{(k+1)n}}
\end{bmatrix} = 
\frac{\partial  \mathcal{L}(x_k, u_k)}{\partial x_k}^\top \lambda_{k+1}
$$

- w.r.t \\( x_N\\) is a \\( n \times 1 \\) column vector, **which is a boundary condition at \\(k = N \\)**.

$$
\frac{\partial  \mathcal{L}(x_N, u_k)}{\partial x_k} = 
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial x_{N1}} \\
\frac{\partial \mathcal{L}}{\partial x_{N2}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial x_{Nn}}
\end{bmatrix}
$$

- We can make the above expression more compact by defining the **Control Hamiltonian** (in discrete time):

$$ H_k (x_k, u_k, \lambda_{k+1}) = L(x_k, u_k) + \lambda_{k+1}^\top f(x_k, u_k) $$

The Langrangian function \eqref{eq:Lagrangian} can be then rewritten as:

$$ \mathcal{L} = L(x_0, u_0) + \lambda_{1}^\top f(x_k, u_k) + \sum_{k=1}^{N-1} \left[ L(x_k, u_k) + \lambda_{k+1}^\top f(x_k, u_k) - \lambda_{k}^\top x_{k+1}  \right] + \Phi(x_N) - \lambda_{N}^\top x_N $$

$$ \mathcal{L} = H_0 (x_0, u_0, \lambda_{1}) + \sum_{k=1}^{N-1} \left[ H_k (x_k, u_k, \lambda_{k+1}) - \lambda_{k}^\top x_k  \right] + \Phi(x_N) - \lambda_{N}^\top x_N$$

The stationarity condition requires the gradient of the Lagrangian with respect to both \(x_k\) and \(u_k\) to be zero:

$$ \nabla_{x_k, u_k} \mathcal{L} = 0 $$

The stationarity condition in the KKT framework involves taking the gradient of the Lagrangian with respect to the decision variables (both \\(x_k\\) and \\(u_k\\)) and setting it to zero. This ensures that the Lagrangian is stationary (i.e., no change) at the optimal solution.

In the context of PMP, the stationarity condition with respect to the control variable \(u_k\) is used to derive the optimality condition for the control:

- **State Equation (Primal Feasibility:):** (rearranging \eqref{eq:state})

$$ x_{k+1}^* = \frac{\partial H_k (x_k^*, u_k^*, \lambda_{k+1}^*)}{\partial \lambda^*}$$

- **Costate Equation (Dual Feasibility & Resulting From Stationarity Condition):** (rearranging \eqref{eq:costate})

$$ \lambda_{k}^* = \frac{\partial H_k (x_k^*, u_k^*, \lambda_{k+1}^*)}{\partial x_k^*}$$

Recall that we denote the initial boundary condition at \\( k = 0 \\) by \\( x_0 \\) and the terminal boundary condition at \\( k = N \\) by \\( \lambda_N \\). The boundary conditions at each intermediate point must also be satisfied. For instance, \\( x_1 \\) exists in two equations (\\( x_1 = f(x_0, u_0) \\) and \\( x_2 = f(x_1, u_1) \\)), and it must be consistent and satisfy both equations. Therefore, we have as many equations as boundary conditions to solve the problem. In addition, notice that \\(\lambda_{k+1}^* \\) is on the right and \\( \lambda_{k}^*\\) is on left. This indicates that <span style="color:red"> the costate equation is evolving backward in time </span>.

- **Optimality Condition (Resulting From Stationarity Condition):**

In the case of there is no control input constraint, we can just take the partial derivative of the Control Hamiltonian w.r.t. \\(u_k\\) and set is to zero:

$$ \frac{\partial H_k^*}{\partial u_k^*} = 0 $$

In the case of control input constraint exits, we need to find the control input within the admissible set that minimize the Control Hamiltonian:

$$ u_k^* = \arg \min_{u_k \in \mathcal{U}} H_k (x_k^*, u_k^*, \lambda_{k+1}^*) $$

## Example 3: Constrained Cotinuous LQR [1]

Consider the LQR in continuous time setting, the objective function to be minimized is defined as follows:

$$ \min_{x(t), u(t)} \frac{1}{2} \int_{0}^{t_f} \left( x^T(t) Q x(t) + u^T(t) R u(t) \right) dt $$

$$ s.t. \quad \dot{x}(t) = A x(t) + B u(t), $$

$$ \quad x(0) = r_0, \quad x(t_f) = r_f, \quad u \in \mathcal{U} $$

where $$Q > 0, R \geq 0$$. In this case, Let's use the Pontryagin's Maximum Principle, so the **Control Hamiltonian (maximum)** is defined as:

$$ H = - \frac{1}{2} x^T Q x - \frac{1}{2} u^T R u + \lambda^T (A x + B u) $$

The optimal $$u^*$$ in case of no control input constraint can be obtained by 

$$ \nabla_{u} \mathcal{H} = 0, \quad u^* = u_{LQR} = -R^{-1}B^T\lambda^*$$

In the case of there exists control input consraints, based on Pontryagin's Maximum Principle:

$$\cancelto{}{-\frac{1}{2} {x^*}^T Q x^{*}} - \frac{1}{2} {u^*}^T R u^* + \cancelto{}{\lambda^{*T} A x^{*}} + \lambda^{*T} B u^*$$

$$
\geq \cancelto{}{-\frac{1}{2} {x^*}^T Q x^{*}} - \frac{1}{2} u^T R u + \cancelto{}{\lambda^{*T} A x^{*}} + \lambda^{*T} B u, \quad u \in \mathcal{U}
$$

$$ - \frac{1}{2} u^{*T} R u^* + \lambda^{*T} B u^* \geq - \frac{1}{2} u^T R u + \lambda^{*T} B u, \quad u \in \mathcal{U} $$

$$ u^* = \arg\max_{u \in \mathcal{U}} \left[ - \frac{1}{2} u^T R u + \lambda^{*T} B u \right] $$

For scalar case (first-ordered and single-input system):

$$ u^*(t) = \arg\max_{u_{\min} \leq u(t) \leq u_{\max}} \left[ - \frac{1}{2} r u(t)^2 + b \lambda^*(t) u(t) \right]$$

$$
u^* = 
\begin{cases} 
\frac{b \lambda^*}{r} & \text{if } u_{\min} \leq \frac{b \lambda^*}{r} \leq u_{\max} \\
u_{\min} & \text{if } \frac{b \lambda^*}{r} < u_{\min} \\
u_{\max} & \text{if } \frac{b \lambda^*}{r} > u_{\max}
\end{cases}
$$

$$ u^*(t) = \text{sat}_{u_{\min}}^{u_{\max}} \left( \frac{b \lambda^*(t)}{r} \right) $$

Substitute into the Hamilton canonical equations (state and costate equations)

$$ \dot{x} = a x + b \, \text{sat} \left( \frac{b \lambda^*(t)}{r} \right) $$

$$ \dot{\lambda} = - a \lambda + q x $$

In general, the opitmal control input cannot be obtained by simply staurate the optimal unconstrained LQR output.

$$ u^* \neq \text{sat}(u_{\text{LQR}}) $$



## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. [L7.1 Pontryagin's principle of maximum (minimum) and its application to optimal control [YouTube]](https://www.youtube.com/watch?v=Bxc4iy2xUjc&list=PLMLojHoA_QPmRiPotD_TnfdUkglTexuqm&index=16&t=1s) (Explains the difference of Control Hamiltonian formulation in both Maximum and Minimum Principle)
 2. [Optimal Control (CMU 16-745) 2023 Lecture 6: Deterministic Optimal Control Intro [YouTube]](https://www.youtube.com/watch?v=U9zrNwMXktQ&list=PLZnJoM76RM6KugDT9sw5zhAmqKnGeoLRa&index=10) (Deriviation of PMP in discrete time setting, starting at 57 minutes and 15 seconds)
 3. [Karush-Kuhn-Tucker (KKT) conditions: motivation and theorem](https://www.youtube.com/watch?v=K3L7UYnZuZ4&list=PLHAS_3-nESXV6XgW53wSkZHazVE7ZkHAV&index=38) (Part of Intro to Optimization Course by Prof. 
Lewis Mitchell, also contains other great explanation including lagrange multipliers, Netwon's method, etc.)
 4. [Hamiltonian Method of Optimization of Control Systems  [YouTube]](https://www.youtube.com/watch?v=r-fscDKfeUs) (Clear example problem solved using Control Hamiltonian)
 5. [Why the Riccati Equation Is important for LQR Control [YouTube]](https://www.youtube.com/watch?v=ZktL3YjTbB4) (Derivation of ARE using the approach of completing the square)
 6. [Matrix Calculus [YouTube]](https://www.youtube.com/watch?v=IgAr5kzza78) (Great explanation on matrix and vector derivatives)
 7. [Geomety of the Pontryagin Maximum Principle  [YouTube]](https://www.youtube.com/watch?v=V04N9X3NxYA&t=9s) (Explanation of PMP from another persepctive)
 8. D. E. Kirk, _Optimal Control Theory: An Introduction, 2004._


## Appendix

The submatrices $$\phi_{11}(t_f - t)$$ and $$\phi_{21}(t_f - t)$$ are parts of the matrix exponential $$e^{A_{\text{aug}}(t_f - t)}$$, so they inherit the exponential decay property. Specifically, both $$\phi_{11}(t_f - t)$$ and $$\phi_{21}(t_f - t)$$ will decay exponentially as $$t_f - t \to \infty$$.


Recall from previous section, after substituing $$u^*$$ and getting the boundary conditions, we obtained:

$$
\begin{bmatrix}
\dot{\mathbf{x}}^*(t) \\
\dot{\mathbf{\lambda}}^*(t)
\end{bmatrix}
=
\begin{bmatrix}
A & -B R^{-1} B^T \\
-Q & -A^T
\end{bmatrix}
\begin{bmatrix}
\mathbf{x}^*(t) \\
\mathbf{\lambda}^*(t)
\end{bmatrix}
$$

$$ \lambda^*(t_f) = Hx^*(t_f)$$

Based on the knowledge of the Linear System Theory, the solution can be expressed as follows:

$$
\begin{bmatrix}
\mathbf{x}^*(t_f) \\
\lambda^*(t_f)
\end{bmatrix} = e^{A_{\text{aug}}(t_f - t)}
\begin{bmatrix}
\mathbf{x^*}(t_f) \\
\lambda^*(t_f)
\end{bmatrix} = 
\begin{bmatrix}
\phi_{11}(t_f - t) & \phi_{12}(t_f - t) \\
\phi_{21}(t_f - t) & \phi_{22}(t_f - t)
\end{bmatrix}
\begin{bmatrix}
\mathbf{x^*}(t) \\
\lambda^*(t)
\end{bmatrix} \\
$$

where

$$A_{\text{aug}} = \begin{bmatrix}
A & -B R^{-1} B^T \\
-Q & -A^T
\end{bmatrix}$$

$$x^*(t_f) = \phi_{11}(t_f - t) x^*(t) + \phi_{12}(t_f - t) \lambda^*(t)$$

$$\lambda^*(t_f) = Hx^*(t_f) = \phi_{21}(t_f - t) x^*(t) + \phi_{22}(t_f - t) \lambda^*(t)$$

Substitute $$x^*(t)$$ into equation of $$\lambda^*(t)$$ and rearrange the equations:

$$ H[\phi_{11}(t_f - t) x^*(t) + \phi_{12}(t_f - t) \lambda^*(t)] = \phi_{21}(t_f - t) x^*(t) + \phi_{22}(t_f - t) \lambda^*(t)$$

$$ H[\phi_{12}(t_f - t) - \phi_{22}(t_f - t)] \lambda^*(t) = [\phi_{21}(t_f - t) - H\phi_{11}(t_f - t)] x^*(t) $$

Thus,

$$\lambda^*(t) = [H\phi_{12}(t_f - t) - \phi_{22}(t_f - t)]^{-1}  [\phi_{21}(t_f - t) - H\phi_{11}(t_f - t)] x^*(t) $$

$$
\Rightarrow \mathbf{u}^*(t) = -R^{-1}B^T \mathbf{P}(t) \mathbf{x}(t) $$

where,

$$
\mathbf{P}(t) = \left[H\phi_{12}(t_f - t) - \phi_{22}(t_f - t)]^{-1}  [\phi_{21}(t_f - t) - H\phi_{11}(t_f - t)\right]
$$

Given that system is stable, we can get the following:

 - **Matrix Exponential Decay:**

   If $$A_{\text{aug}}$$ is stable, the matrix exponential $$e^{A_{\text{aug}}(t_f - t)}$$ decays exponentially as $$t_f - t$$ increases. This implies that the submatrices $$\phi_{ij}(t_f - t)$$ derived from $$e^{A_{\text{aug}}(t_f - t)}$$ also decay exponentially.

 - **Invertibility of the Matrix:**

   The stability and decay properties of the submatrices contribute to the invertibility of $$H \phi_{12}(t_f - t) - \phi_{22}(t_f - t)$$. For the inverse to exist and be well-behaved, the matrix must remain nonsingular (i.e., it must not approach a singularity as $$t_f - t$$ increases).

Thus, these imply that $$P(t)$$ converges to a steady-state value and validate our assumption in the previous section that deriving **ARE**.

