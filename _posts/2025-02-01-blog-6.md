---
title: "From Q-Learning to Deep Q-Learning and Deep Deterministic Policy Gradient"
date: 2025-02-01
permalink: /posts/blog6
tags:
  - Reinforcement Learning
  - (Deep) Q-learning
  - Deep Deterministic Policy Gradient
---
Q-learning, an off-policy reinforcement learning algorithm, relies on the Bellman equation to iteratively update
state-action values. It helps an agent determine the best actions to take in order to maximize thie cumulative
reward. In the context of Q-learning, the Bellman equation expresses the relationship between the current
value of a state-action pair and the future expected rewards. Moving forward, Deep Q-learning extends Q-learning
by utilizing deep neural networks to approximate the Q-values,, enabling it to tackle high-dimensional state
spaces. Another advanced technique, Deep Deterministic Policy Gradient(DDPG), is particularly useful for
environment with continuous action space. This method combines the strengths of Q-learning with policy Gradient
approaches. IN this blog, we will delve into the practicle implementation of these algorithms, applying them to the cartpole
task in Isaccgym environment. Corresponding code can be found at this [repository](https://github.com/lihanlian).

## Bellman Equation

In [one of the other blogs](https://lihanlian.github.io/posts/blog1) on Euler-Lagrange equation, I went through the derivation process for the case of fixed final state and fixed final time. Now, Let's consider more general cases that involve free final state and free final time. To start with, recall the goal is minimize the objective functional as follows:

$$ J[x] = \int_0^{t_f} L(x(t), \dot{x}(t)) \, dt $$

The first varition of \\(J[x]\\) can be written as:

$$ \delta J[x] = \int_0^{t_f} \left( \frac{\partial L}{\partial x} \delta x + \frac{\partial L}{\partial \dot{x}} \delta \dot{x} \right) dt $$

For the optimal solution, the first varition of the functional \\(J[x]\\) should be zero. Note that in the later section, for the sake of concise notation, I will use $$ L^* \big|_{t_f} $$ to denote \\(L^*(x(t_f), \dot{x}(t_f))\\). 
Also keep in mind that I will denote \\(x(t)\\), \\(\dot{x}(t)\\)  as  $$x$$, $$\dot{x}$$ for simplicity too. 


## Q-Learnig


## Deep Q-Learnig

## Deep Deterministic Policy Gradient (DDPG)



## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fab fa-youtube"></i> [L7.1 Pontryagin's principle of maximum (minimum) and its application to optimal control](https://www.youtube.com/watch?v=Bxc4iy2xUjc&list=PLMLojHoA_QPmRiPotD_TnfdUkglTexuqm&index=16&t=1s) (Explains the difference of Control Hamiltonian formulation in both Maximum and Minimum Principle)
 2. <i class="fab fa-youtube"></i> [Optimal Control (CMU 16-745) 2023 Lecture 6: Deterministic Optimal Control Intro](https://www.youtube.com/watch?v=U9zrNwMXktQ&list=PLZnJoM76RM6KugDT9sw5zhAmqKnGeoLRa&index=10) (Deriviation of PMP in discrete time setting, starting at 57 minutes and 15 seconds)
 3. <i class="fab fa-youtube"></i> [Karush-Kuhn-Tucker (KKT) conditions: motivation and theorem](https://www.youtube.com/watch?v=K3L7UYnZuZ4&list=PLHAS_3-nESXV6XgW53wSkZHazVE7ZkHAV&index=38) (Part of Intro to Optimization Course by Prof. 
Lewis Mitchell, also contains other great explanation including lagrange multipliers, Netwon's method, etc.)
 4. <i class="fab fa-youtube"></i> [Hamiltonian Method of Optimization of Control Systems ](https://www.youtube.com/watch?v=r-fscDKfeUs) (Clear example problem solved using Control Hamiltonian)
 5. <i class="fab fa-youtube"></i> [Why the Riccati Equation Is important for LQR Control](https://www.youtube.com/watch?v=ZktL3YjTbB4) (Derivation of ARE using the approach of completing the square)
 6. <i class="fab fa-youtube"></i> [Matrix Calculus](https://www.youtube.com/watch?v=IgAr5kzza78) (Great explanation on matrix and vector derivatives)
 7. <i class="fab fa-youtube"></i> [Geomety of the Pontryagin Maximum Principle ](https://www.youtube.com/watch?v=V04N9X3NxYA&t=9s) (Explanation of PMP from another persepctive)
 8. <i class="fa-solid fa-book-open"></i> D. E. Kirk, _Optimal Control Theory: An Introduction, 2004._


