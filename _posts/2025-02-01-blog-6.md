---
title: "From Q-Learning to Deep Q-Learning and Deep Deterministic Policy Gradient"
date: 2025-02-01
permalink: /posts/blog6
tags:
  - Reinforcement Learning
  - (Deep) Q-learning
  - Deep Deterministic Policy Gradient
---
[**_Q-learning_**](https://en.wikipedia.org/wiki/Q-learning), an **off-policy** reinforcement learning algorithm, uses the [**_Bellman equation_**](https://en.wikipedia.org/wiki/Bellman_equation) to iteratively update state-action values, helping an agent determine the best actions to maximize cumulative rewards. Deep Q-learning improves upon Q-learning by leveraging deep Q network (DQN) to approximate Q-values, enabling it to handle continuous state spaces but it is still only suitable for **discrete action spaces**. Further advancement, [**_Deep Deterministic Policy Gradient (DDPG)_**](https://arxiv.org/abs/1509.02971), combines Q-learning's principles with **policy gradients**, making it also suitable for continuous action spaces. This blog starts by discussing the basic components of reinforcement learning and gradually explore how Q-learning evolves into DQN and DDPG, with application for solving the cartpole environment in [**_Isaacgym simulator_**](https://github.com/isaac-sim/IsaacGymEnvs). Corresponding code can be found at this <i class="fa-brands fa-github"></i> [repository](https://github.com/lihanlian).

## Reinforcement Learning Overview 

The goal of reinforcement learning is to train an agent to interact with an environment to maximize cumulative rewards over time. Unlike supervised learning, RL does not rely on labeled data but instead learns from feedback in the form of rewards or penalties.

In contrast to typical optimal control problems, which use differential equations to describe system dynamics, reinforcement learning problems are typically modeled as **MDPs**, which can sometimes be beneficial in terms of taking model uncertainty into account. An MDP is defined by the tuple **$$(S, A, P, R, γ)$$:**

- **$$S$$**: Set of states.
- **$$A$$**: Set of actions.
- **$$P(s' \mid s, a)$$**: Transition probability to **$$s'$$** given state **$$s$$** and action **$$a$$**.
- **$$R(s, a)$$**: Reward function for taking action **$$a$$** in state **$$s$$**.
- **$$γ$$**: Discount factor to prioritize short-term vs. long-term rewards.

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/rl-framework.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "What Is Reinforcement Learning?" (Mathwork)</figcaption>
</figure>

The picture above shows a general representation of reinforcement learning scenario and illustrate the necessary component which includes:

1. **Agent**: Learns the policy (**π**) to maximize rewards.
2. **Environment**: The system the agent interacts with.
3. **Reward Signal**: Feedback to guide the agent.
4. **Policy**: Maps states to actions ($$\pi (a \mid s)$$).
5. **Value / Q Function**: Estimates the "goodness" of states or state-action pairs.


 - ### Bellman Equation

    The Bellman equation is the cornerstone of reinforcement learning, describing the relationship between the value of a state and the values of its successor states. It is a recursive decomposition of the value function.

    The Bellman equation expresses $$ V^\pi(s) $$ recursively:

    $$ V^\pi(s) = \sum_a \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^\pi(s') \right]$$

    **Where:**
    - $$ P(s' \mid s, a) $$: Transition probability to state $$ s' $$ after taking action $$ a $$.
    - $$ R(s, a) $$: Immediate reward.

    The value of a state $$ s $$ under a policy $$ \pi $$ is the expected cumulative reward starting from $$ s $$, following $$ \pi $$:

    $$ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \; \middle| \; s_0 = s \right] $$

    **Here:**
    - $$ r_t $$: Reward at time step $$ t $$.
    - $$ \gamma $$: Discount factor ($$ 0 \leq \gamma \leq 1 $$).
    - $$ \pi(a \mid s) $$: Probability of taking action $$ a $$ in state $$ s $$ under policy $$ \pi $$.

  - ### Bellman Optimality Equation (Optimal Value Function)

    For the optimal policy $$ \pi^* $$, the Bellman equation becomes:

    $$ V^*(s) = \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^*(s') \right] $$

    The Bellman equation provides the foundation for dynamic programming methods like value iteration and policy iteration.

- ### Q Function and Value Function

  The value function and Q-function represent different perspectives on evaluating the quality of states and actions:

  Value Function ($$ V^\pi(s) $$)

  Measures the expected cumulative reward starting from a state $$ s $$ and following a policy $$ \pi $$:

  $$ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \; \middle| \; s_0 = s \right] $$


  Q-Function ($$ Q^\pi(s, a) $$)

  Extends the value function by also considering the action $$ a $$ taken in $$ s $$ before following $$ \pi $$:

  $$ Q^\pi(s, a) = \mathbb{E}_\pi \left[ r_0 + \gamma V^\pi(s_1) \; \middle| \; s_0 = s, a_0 = a \right] $$

  Or equivalently:

  $$ Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^\pi(s'). $$


  Relationship Between $$ Q(s, a) $$ and $$ V(s) $$

  The value function can be derived from the Q-function:

  $$ V^\pi(s) = \sum_a \pi(a \mid s) Q^\pi(s, a).$$

  For the optimal policy:

  $$ V^*(s) = \max_a Q^*(s, a). $$


 - ### Value Iteration and Policy Iteration

 - ### Temporal Difference (TD) Learning

 - ### Analogy with Model Predictive Control (MPC)

## Q-Learnig

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/q-learning.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "Deep Q-Learning" (GeeksforGeeks)</figcaption>
</figure>

- ### Replay Buffer

- ### Target Network


## Deep Q-Learnig

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/deep-q-learning.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "Deep Q-Learning" (GeeksforGeeks)</figcaption>
</figure>

## Deep Deterministic Policy Gradient (DDPG)



## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fa-solid fa-blog"></i> [Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html) (OpenAI Spinning Up)
 2. <i class="fab fa-youtube"></i> [L21: Temporal Difference Learning](https://www.youtube.com/watch?v=XCINqY640mE&t=8s)
 3. <i class="fab fa-youtube"></i> [Q-Learning Explained - A Reinforcement Learning Technique](https://www.youtube.com/watch?v=qhRNvCVVJaA&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=6) & [Deep Q-Learning - Combining Neural Networks and Reinforcement Learning](https://www.youtube.com/watch?v=wrBUkpiRvCA&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=11) (Deep Lizard RL Series)
 4. <i class="fab fa-youtube"></i> [Reinforcement Learning - "DDPG" explained](https://www.youtube.com/watch?v=oydExwuuUCw) and [M16V06 Deep deterministic policy gradient](https://www.youtube.com/watch?v=dBZXv7yzG64)
 5. <i class="fa-brands fa-github"></i> [minimal-isaac-gym](https://github.com/lorenmt/minimal-isaac-gym), <i class="fa-brands fa-github"></i> [DDPG/pytorch/lunar-lander](https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/DDPG/pytorch/lunar-lander) and its <i class="fab fa-youtube"></i> [implementation walkthrough](https://www.youtube.com/watch?v=6Yd5WnYls_Y)
