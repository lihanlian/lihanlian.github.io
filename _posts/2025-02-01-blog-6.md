---
title: "From Q-Learning to Deep Q-Learning and Deep Deterministic Policy Gradient"
date: 2025-02-01
permalink: /posts/blog6
tags:
  - Reinforcement Learning
  - (Deep) Q-learning
  - Deep Deterministic Policy Gradient
---
[**_Q-learning_**](https://en.wikipedia.org/wiki/Q-learning), an **off-policy** reinforcement learning algorithm, uses the [**_Bellman equation_**](https://en.wikipedia.org/wiki/Bellman_equation) to iteratively update state-action values, helping an agent determine the best actions to maximize cumulative rewards. Deep Q-learning improves upon Q-learning by leveraging deep Q network (DQN) to approximate Q-values, enabling it to handle continuous state spaces but it is still only suitable for **discrete action spaces**. Further advancement, [**_Deep Deterministic Policy Gradient (DDPG)_**](https://arxiv.org/abs/1509.02971), combines Q-learning's principles with **policy gradients**, making it also suitable for continuous action spaces. This blog starts by discussing the basic components of reinforcement learning and gradually explore how Q-learning evolves into DQN and DDPG, with application for solving the cartpole environment in [**_Isaacgym simulator_**](https://github.com/isaac-sim/IsaacGymEnvs). Corresponding code can be found at this <i class="fa-brands fa-github"></i> [repository](https://github.com/lihanlian).

## Reinforcement Learning Overview 

The goal of reinforcement learning is to train an agent to interact with an environment to maximize cumulative rewards over time. Unlike supervised learning, RL does not rely on labeled data but instead learns from feedback in the form of rewards or penalties.

In contrast to typical optimal control problems, which use differential equations to describe system dynamics, reinforcement learning problems are typically modeled as **MDPs**, which can sometimes be beneficial in terms of taking model uncertainty into account. An MDP is defined by the tuple **$$(S, A, P, R, γ)$$:**

- **$$S$$**: Set of states.
- **$$A$$**: Set of actions.
- **$$P(s' \mid s, a)$$**: Transition probability to **$$s'$$** given state **$$s$$** and action **$$a$$**.
- **$$R(s, a)$$**: Reward function for taking action **$$a$$** in state **$$s$$**.
- **$$γ$$**: Discount factor to prioritize short-term vs. long-term rewards.

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/rl-framework.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "What Is Reinforcement Learning?" (Mathwork)</figcaption>
</figure>

The picture above shows a general representation of reinforcement learning scenario and illustrate the necessary component which includes:

1. **Agent**: Learns the policy (**π**) to maximize rewards.
2. **Environment**: The system the agent interacts with.
3. **Reward Signal**: Feedback to guide the agent.
4. **Policy**: Maps states to actions ($$\pi (a \mid s)$$).
5. **Value / Q Function**: Estimates the "goodness" of states or state-action pairs.


 - ### Bellman Equation

    The Bellman equation is the cornerstone of reinforcement learning, describing the relationship between the value of a state and the values of its successor states. It is a recursive decomposition of the value function.

    The Bellman equation expresses $$ V^\pi(s) $$ recursively:

    $$ V^\pi(s) = \sum_a \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^\pi(s') \right]$$

    **Where:**
    - $$ P(s' \mid s, a) $$: Transition probability to state $$ s' $$ after taking action $$ a $$.
    - $$ R(s, a) $$: Immediate reward.

    The value of a state $$ s $$ under a policy $$ \pi $$ is the expected cumulative reward starting from $$ s $$, following $$ \pi $$:

    $$ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \; \middle| \; s_0 = s \right] $$

    **Here:**
    - $$ r_t $$: Reward at time step $$ t $$.
    - $$ \gamma $$: Discount factor ($$ 0 \leq \gamma \leq 1 $$).
    - $$ \pi(a \mid s) $$: Probability of taking action $$ a $$ in state $$ s $$ under policy $$ \pi $$.

  - ### Bellman Optimality Equation (Optimal Value Function)

    For the optimal policy $$ \pi^* $$, the Bellman equation becomes:

    $$ V^*(s) = \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^*(s') \right] $$

    The Bellman equation provides the foundation for dynamic programming methods like value iteration and policy iteration.

- ### Q Function and Value Function

  The value function and Q-function represent different perspectives on evaluating the quality of states and actions:

  Value Function ($$ V^\pi(s) $$)

  Measures the expected cumulative reward starting from a state $$ s $$ and following a policy $$ \pi $$:

  $$ V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \; \middle| \; s_0 = s \right] $$


  Q-Function ($$ Q^\pi(s, a) $$)

  Extends the value function by also considering the action $$ a $$ taken in $$ s $$ before following $$ \pi $$:

  $$ Q^\pi(s, a) = \mathbb{E}_\pi \left[ r_0 + \gamma V^\pi(s_1) \; \middle| \; s_0 = s, a_0 = a \right] $$

  Or equivalently:

  $$ Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^\pi(s'). $$


  Relationship Between $$ Q(s, a) $$ and $$ V(s) $$

  The value function can be derived from the Q-function:

  $$ V^\pi(s) = \sum_a \pi(a \mid s) Q^\pi(s, a).$$

  For the optimal policy:

  $$ V^*(s) = \max_a Q^*(s, a). $$


 - ### Value Iteration and Policy Iteration

  Value iteration and policy iteration are **dynamic programming** methods used to solve **Markov Decision Processes (MDPs)** by computing the optimal value function and policy.

  - Value Iteration

    Value iteration is an approach to compute the optimal value function $$ V^*(s) $$ by applying the **Bellman Optimality Equation** iteratively. It avoids explicitly maintaining a policy during updates and directly computes $$ V^*(s) $$, from which the optimal policy can be derived.

  - Bellman Optimality Update Rule:

  $$ V_{k+1}(s) = \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V_k(s') \right] $$

  - This is an iterative update that refines $$ V(s) $$ over time.
  - Once $$ V^*(s) $$ converges, the optimal policy is derived as:

  $$ \pi^*(s) = \arg\max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^*(s') \right] $$

  **Pros**: More direct than policy iteration.  
  **Cons**: Convergence can be slow; requires full knowledge of the MDP.


  - Policy Iteration

  Policy iteration alternates between **policy evaluation** (computing $$ V^\pi(s) $$ for a fixed policy) and **policy improvement** (updating the policy based on $$ Q^\pi(s, a) $$).

  1. **Policy Evaluation**: Compute $$ V^\pi(s) $$ for a given policy $$ \pi $$.
    - Solve the Bellman Expectation Equation:

      $$
      V^\pi(s) = \sum_a \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^\pi(s') \right]
      $$

  2. **Policy Improvement**: Update policy greedily:

      $$
      \pi'(s) = \arg\max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma V^\pi(s') \right]
      $$

  3. **Repeat** until policy convergence ($$ \pi' = \pi $$).

  **Pros**: Typically converges faster than value iteration in discrete MDPs.  
  **Cons**: Requires explicit policy storage and evaluation.

---

## Comparison Between Value Iteration and Policy Iteration

| Aspect                | Value Iteration | Policy Iteration |
|-----------------------|----------------|------------------|
| **Approach**          | Iteratively updates $$ V(s) $$ | Iterates between policy evaluation and improvement |
| **Policy Updates**    | Derived at the end | Updated iteratively |
| **Convergence**       | Can take more iterations | Fewer iterations for small problems |
| **Computational Cost** | Can be more efficient for large state spaces | More expensive per iteration |

---

 - Temporal Difference (TD) Learning

  Temporal Difference (TD) Learning is a fundamental technique in reinforcement learning that combines aspects of **Monte Carlo methods** (learning from experience) and **dynamic programming** (bootstrapping with value estimates). TD learning is at the core of **Q-learning** and **Deep Q Networks (DQN)**.

  TD learning updates value estimates incrementally using **bootstrapping**, meaning it updates values using estimates of future values rather than waiting for the final reward.

  - TD Update Rule for Value Function:

  $$ V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right] $$

  - $$ r_t $$ = reward received.
  - $$ V(s_{t+1}) $$ = estimate of the next state's value.
  - $$ \alpha $$ = learning rate.

This update is performed **immediately after each transition**, rather than waiting for the entire episode to end.

---

  - 2.3 Q-Learning as a TD Method

  Q-learning extends TD learning by estimating the optimal action-value function $$ Q(s, a) $$ using the **Bellman Optimality Equation**:

  $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right] $$

  ### How TD Learning Relates to Q-Learning:

  - TD Learning updates value estimates $$ V(s) $$, while Q-learning updates action-value estimates $$ Q(s, a) $$.
  - Q-learning is an **off-policy** method because it updates $$ Q(s, a) $$ using the best action $$ \max_{a'} Q(s', a') $$ rather than the action from the current policy.
  - **Deep Q Networks (DQN)** extend Q-learning by using **neural networks** to approximate $$ Q(s, a) $$.

---

 - ### Analogy with Model Predictive Control (MPC)

  Model Predictive Control (MPC) is an optimization-based control technique that iteratively solves a **finite-horizon optimal control problem**, making it conceptually similar to reinforcement learning.

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog6/rl-control.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "Reinforcement Learning for Control Systems Applications" (Mathwork)</figcaption>
</figure>

  * How Q-Function and Value Function Relate to MPC

  - **Q-function** ($$ Q(s, a) $$) is somewhat analogous to the stage cost $$ \ell(x_t, u_t) $$ in MPC.
    - $$ Q(s, a) $$ represents the **immediate reward** for taking action $$ a $$ in state $$ s $$, plus the expected **future cumulative reward**.
    - In MPC, the cost function includes a **stage cost** (immediate penalty) and a **terminal cost**.

  - **Value function** ($$ V(s) $$) is analogous to the terminal cost $$ \phi(x_T) $$ in MPC.
    - $$ V(s) $$ represents the **cost-to-go** from state $$ s $$.
    - In MPC, the terminal cost function ensures stability and optimal performance beyond the finite horizon.


## Q-Learnig

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/q-learning.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "Deep Q-Learning" (GeeksforGeeks)</figcaption>
</figure>

- ### Replay Buffer

- ### Target Network


## Deep Q-Learnig

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/deep-q-learning.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "Deep Q-Learning" (GeeksforGeeks)</figcaption>
</figure>

## Deep Deterministic Policy Gradient (DDPG)



## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fa-solid fa-blog"></i> [Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html) (OpenAI Spinning Up)
 2. <i class="fab fa-youtube"></i> [L21: Temporal Difference Learning](https://www.youtube.com/watch?v=XCINqY640mE&t=8s)
 3. <i class="fab fa-youtube"></i> [Q-Learning Explained - A Reinforcement Learning Technique](https://www.youtube.com/watch?v=qhRNvCVVJaA&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=6) & [Deep Q-Learning - Combining Neural Networks and Reinforcement Learning](https://www.youtube.com/watch?v=wrBUkpiRvCA&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=11) (Deep Lizard RL Series)
 4. <i class="fab fa-youtube"></i> [Reinforcement Learning - "DDPG" explained](https://www.youtube.com/watch?v=oydExwuuUCw) and [M16V06 Deep deterministic policy gradient](https://www.youtube.com/watch?v=dBZXv7yzG64)
 5. <i class="fa-brands fa-github"></i> [minimal-isaac-gym](https://github.com/lorenmt/minimal-isaac-gym), <i class="fa-brands fa-github"></i> [DDPG/pytorch/lunar-lander](https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/DDPG/pytorch/lunar-lander) and its <i class="fab fa-youtube"></i> [implementation walkthrough](https://www.youtube.com/watch?v=6Yd5WnYls_Y)
