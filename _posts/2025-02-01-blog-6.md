---
title: "From Q-Learning to Deep Q-Learning and Deep Deterministic Policy Gradient"
date: 2025-02-01
permalink: /posts/blog6
tags:
  - Reinforcement Learning
  - (Deep) Q-learning
  - Deep Deterministic Policy Gradient
---
Q-learning, an off-policy reinforcement learning algorithm, relies on the Bellman equation to iteratively update
state-action values. It helps an agent determine the best actions to take in order to maximize thie cumulative
reward. In the context of Q-learning, the Bellman equation expresses the relationship between the current
value of a state-action pair and the future expected rewards. Moving forward, Deep Q-learning extends Q-learning
by utilizing deep neural networks to approximate the Q-values,, enabling it to tackle high-dimensional state
spaces. Another advanced technique, Deep Deterministic Policy Gradient(DDPG), is particularly useful for
environment with continuous action space. This method combines the strengths of Q-learning with policy Gradient
approaches. IN this blog, we will delve into the practicle implementation of these algorithms, applying them to the cartpole
task in Isaccgym environment. Corresponding code can be found at this [repository](https://github.com/lihanlian).

## Bellman Equation

In [one of the other blogs](https://lihanlian.github.io/posts/blog1) on Euler-Lagrange equation, I went through the derivation process for the case of fixed final state and fixed final time. Now, Let's consider more general cases that involve free final state and free final time. To start with, recall the goal is minimize the objective functional as follows:

$$ J[x] = \int_0^{t_f} L(x(t), \dot{x}(t)) \, dt $$

The first varition of \\(J[x]\\) can be written as:

$$ \delta J[x] = \int_0^{t_f} \left( \frac{\partial L}{\partial x} \delta x + \frac{\partial L}{\partial \dot{x}} \delta \dot{x} \right) dt $$

For the optimal solution, the first varition of the functional \\(J[x]\\) should be zero. Note that in the later section, for the sake of concise notation, I will use $$ L^* \big|_{t_f} $$ to denote \\(L^*(x(t_f), \dot{x}(t_f))\\). 
Also keep in mind that I will denote \\(x(t)\\), \\(\dot{x}(t)\\)  as  $$x$$, $$\dot{x}$$ for simplicity too. 

## Value Iteration and Policy Iteration


## Q-Learnig


## Deep Q-Learnig

## Deep Deterministic Policy Gradient (DDPG)



## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fa-solid fa-blog"></i> [Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html) (OpenAI Spinning Up)
 2. <i class="fab fa-youtube"></i> [Reinforcement Learning - "DDPG" explained](https://www.youtube.com/watch?v=oydExwuuUCw)
 3. <i class="fab fa-youtube"></i> [Karush-Kuhn-Tucker (KKT) conditions: motivation and theorem](https://www.youtube.com/watch?v=K3L7UYnZuZ4&list=PLHAS_3-nESXV6XgW53wSkZHazVE7ZkHAV&index=38) (Part of Intro to Optimization Course by Prof. 
Lewis Mitchell, also contains other great explanation including lagrange multipliers, Netwon's method, etc.)

