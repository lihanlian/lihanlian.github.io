---
title: 'Dwell on Differential Dynamic Programming (DDP) and Iterative Linear Quadratic Regulator (iLQR)'
date: 2024-08-31
permalink: /posts/2024/08/31
tags: [Trajectory Optimization, Dynamic Programming, Reinforcement Learning]
---
Although optimal control and reinforcement learning appear to be distinct field, they are, in fact closely related. [_Differential Dynamic Programming (DDP)_](https://en.wikipedia.org/wiki/Differential_dynamic_programming) and **_Iterative Linear Quadratic Regulator (iLQR)_**, two powerful algorithms commonly utilized in trajectory optimizations, exemplify how model-based reinforcement learning can bridge the gap between these domains. This blog begins by discussing the fundational principles, including [**_Newton's method_**](https://en.wikipedia.org/wiki/Newton%27s_method) and [**_Bellman Equation_**](https://en.wikipedia.org/wiki/Bellman_equation). It then delves into the specifics of the DDP and iLQR algorithms, illustrating their application through the classical problem of double pendulum swing-up control.

## Newton's Method and Line Search

   Netwon's Method is an iterative optimization algorithm primarily employed for root finding and unconstrained optimization problems. It is a valuable numerical approach that leverages second-order derivative information, especially when the analytical solutions are impractical. [Line search](https://en.wikipedia.org/wiki/Line_search) is a technique that used in optimization to determine a suitable step size, ensuring appropriate update. It is often used in conjunction with gradient-based methods including Newton's Method.

 - **Newton's Method for Root Finding**

    - Updating Equation:

        The goal of the root finding problem for a function $$f(x)$$ is to find the $$x$$ such that $$f(x) = 0$$. The equation used for iteratively updating the guess of $$x$$ is as follows:

        $$ x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

    - Derivation:

    1. Start with approximating the function $$f(x)$$ by first-order Talyor expansion at $$x_n$$: 

        $$ f(x) \approx f(x_n) + f'(x_n)(x - x_n) $$

    2. Since the the goal is to find the root of $$f(x)$$ , set $$f(x)$$ to $$0$$ using the approximated expression:

        $$ 0 \approx f(x_n) + f'(x_n)(x_{n+1} - x_n) $$

    3. Solving for $$x_{n+1}$$:

        $$ x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} $$

 - ### Newton's Method for Optimization

    Newton's method for optimization seeks to find the minimum or maximum of a function $$f(x)$$. This involves solving $$\nabla f(x) = 0$$, where $$\nabla f(x)$$ is the gradient.

    - Updating Equation:

    $$ x_{n+1} = x_n - H_f(x_n)^{-1}\nabla f(x_n) $$

    - Derivation:

    1. Consider the Taylor series expansion of $$f(x)$$ around $$x_n$$:

        $$ f(x) \approx f(x_n) + \nabla f(x_n)^T(x - x_n) + \frac{1}{2}(x - x_n)^T H_f(x_n)(x - x_n) $$

    2. For optimization, we want $$\nabla f(x) = 0$$:

        $$ \nabla f(x) \approx \nabla f(x_n) + H_f(x_n)(x_{n+1} - x_n) $$

    3. Solving for $$x_{n+1}$$:

        $$ 0 = \nabla f(x_n) + H_f(x_n)(x_{n+1} - x_n) $$

        $$ x_{n+1} = x_n - H_f(x_n)^{-1}\nabla f(x_n) $$


 - ### Newton's Method with Line Search


## Bellman's Principle of Optimality 

## iLQR Algorithms

## DDP Algorithms

## Example:

## Summary

- **Root Finding**: Uses the first derivative $$f'(x)$$. The method in the picture shows:
    $$
    x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
    $$

- **Optimization**: Uses both the gradient $$\nabla f(x)$$ and the Hessian $$H_f(x)$$ for multi-dimensional problems. In one dimension, the Hessian is simply $$f''(x)$$, but in general optimization problems, it involves the second-order partial derivatives.

### References
 - [CS 285: Lecture 10, Part 4 [YouTube]](https://www.youtube.com/watch?v=-hO-AnFYm6M&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=8&t=929s) (Part of Deep Reinforcement Learning Open Course at UC Berkeley)
 - [Optimal Control (CMU 16-745) 2023 Lecture 11: Differential Dynamic Programming](https://www.youtube.com/watch?v=hUf5YhSptLs&list=PLZnJoM76RM6KugDT9sw5zhAmqKnGeoLRa&index=18) (Part of optimal control open course from CMU)
 - Excellent resources from Cornell: [lecture slides](https://www.cs.cornell.edu/courses/cs6756/2022fa/assets/slides_notes/lec6_slides.pdf) & [notes](https://wensun.github.io/CS4789_data/Iterative_LQR-3.pdf)
 - [Iterative linear quadratic regulator [YouTube]](https://www.youtube.com/watch?v=ryu0BbE4nb8&list=PLyXDCTF4yPcQ1GozC3vPmrJuN-icTFOW0&index=24) (Include the connection with Pontryagin's Maximum Principle)
 - [RL â€” LQR & iLQR Linear Quadratic Regulator [Blog Post]](https://jonathan-hui.medium.com/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750)