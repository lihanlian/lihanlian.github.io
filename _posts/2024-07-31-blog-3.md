---
title: 'Implementation Details of Basic-and-Fast-Neural-Style-Transfer'
date: 2024-07-31
permalink: /posts/2024/07/31
tags:
  - Computer Vision
  - Neural Style Transfer
  - Deep Learning
---
[**_Neural style transfer (NST)_**](https://en.wikipedia.org/wiki/Neural_style_transfer) is a valuable learning tool for anyone interested in deep learning, encompassing essential components such as [**_convolutional neural networks (CNNs)_**](https://en.wikipedia.org/wiki/Convolutional_neural_network), VGG network, upsampling and [**_residual networks_**](https://en.wikipedia.org/wiki/Residual_neural_network). The basic neural style transfer method captures and manipulates image features using CNNs and the VGG network directly on the input image. In contrast, the fast neural style transfer method uses dataset to train an inference neural network that can be used for real-time style transfer. This blogs contains the explanation for the implementation of both methods. Corresponding repository can be found [here (Github)](https://github.com/lihanlian/basic-and-fast-neural-style-transfer). 

## Basic NST

 - ### Method Overview

 ## Loss Functions:

1. **Content Loss:** Measures how much the content in the generated image differs from the content in the content image.

   $$
   \text{Content Loss} = \frac{1}{2} \sum_{i,j} \left( F_{ij}^{C} - F_{ij}^{G} \right)^2
   $$

   where \( F_{ij}^{C} \) and \( F_{ij}^{G} \) are the feature representations of the content image and the generated image, respectively.

2. **Style Loss:** Measures how much the style in the generated image differs from the style in the style image using Gram matrices.

   $$
   \text{Style Loss} = \frac{1}{4N^2M^2} \sum_{i,j} \left( G_{ij}^{S} - G_{ij}^{G} \right)^2
   $$

   where \( G_{ij}^{S} \) and \( G_{ij}^{G} \) are the Gram matrices of the style image and the generated image, respectively.


 - ### VGG-19

 VGG (Visual Geometry Group) Network is a deep convolutional neural network architecture developed by the Visual Geometry Group at the University of Oxford. **VGG-19** is used in this implementation and it is one of the variants, characterized by its depth and simplicity in terms of the use of **small (3x3) convolution filters**. The number "19" indicates the network has 19 layers with learnable parameters, including 16 convolutional layers, 3 fully connected layers, and 5 max-pooling layers. Below is the structure information of **VGG-19** printed out from pytorch:

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog3/vgg19.png' style="width: 100%;">
  <figcaption style="text-align: center;">Information of each layers of VGG-19</figcaption>
</figure>

 - Convolutional Layers: Extract spatial features from the input image.
 - Max-Pooling Layers: Downsample the feature maps, reducing the spatial dimensions.
 - Fully Connected Layers: Act as classifiers (usually used in the original VGG for classification tasks).

 ## Reasons for Choosing Specific Layers

- The selected layers are not specifically the max-pooling layers; they are a combination of layers at different depths.
- The choice of layers ensures that we capture both fine-grained details (from lower layers) and high-level patterns (from higher layers).
- This combination helps in effectively transferring the style while maintaining the content structure.
In the context of neural style transfer, the VGG19 network is used as a feature extractor. Specifically, we use the feature maps from certain layers of VGG19 to compute the content and style losses.

Here's a detailed explanation of how this is done:

### 1. Selecting Specific Layers

We select specific layers from VGG19 to compute the content and style losses. In the provided code, these layers are:

- Layer 0: Conv1_1
- Layer 5: Conv2_1
- Layer 10: Conv3_1
- Layer 19: Conv4_1
- Layer 28: Conv5_1

These layers are chosen because:

- **Lower Layers** (e.g., Conv1_1) capture basic features like edges and textures.
- **Higher Layers** (e.g., Conv4_1, Conv5_1) capture more abstract features and structures.

By using a combination of these layers, we can effectively capture the style at different levels of abstraction and the content at a higher level.


 - ### Results

 Style images:

<figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
  <img src='/images/blog/blog3/monet.jpg' style="width: 30%;">
  <img src='/images/blog/blog3/van_gogh.jpg' style="width: 32%;">
  <img src='/images/blog/blog3/picasso.jpg' style="width: 31%;">
  <figcaption style="text-align: center;">Used style images from portrait of different artists: Claude Monet, Vincent van Gogh, Pablo Picasso (from let to right)</figcaption>
</figure>

Content images:

## Fast NST

 - ### Method Overview

 - ### Residual Block

 Regarded as one of the cornerstones in the field of deep learning, **residual neural network (ResNet)** introduces skip connections, which allow the input to bypass certain layers and be added directly to the output of a deeper layer. This technique addresses the vanishing gradient problem, enabling the training of much deeper networks by ensuring gradients can flow through the network more effectively. 
 
 In the context of neural style transfer, residual blocks help retain the content and structure of the original image while allowing the network to apply the style transformation. The skip connections ensure that even as the image undergoes various transformations, the original content information is preserved and added back to the transformed features.

 - ### UpsampleConv Layer

 Based on the pytorch official implementation on fast NST \[4\]: "Upsamples the input and then does a convolution. This method gives better results compared to ConvTranspose2d". The first component of this module is the **upsampling layers**, which are used to increase the spatial resolution of feature maps. There are several methods to achieve upsampling, including:
  - Nearest Neighbor Interpolation: Simply repeats the nearest pixel values.
  - Bilinear/Trilinear Interpolation: Performs linear interpolation over two or three dimensions.
  - Transposed Convolution: Also known as deconvolution, it applies learned filters to upsample the input.

 UpsampleConvLayer combines an upsampling operation **followed by a convolution** to refine the upsampled feature maps, helping to reduce artifacts like checkerboard patterns. In neural style transfer, the role of UpsampleConv Layer can be summarized as follows:
  - Resolution Restoration: They increase the spatial resolution of the feature maps back to the original image size.
  - Detail Enhancement: Combined with convolution, they help refine the upsampled features to enhance image details and reduce artifacts.

 - ### Results

## References
 1. [Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "A Neural Algorithm of Artistic Style." arXiv preprint arXiv:1508.06576 (2015)](https://arxiv.org/abs/1508.06576) (Basic NST Paper)
 2. [Neural Style Transfer (NST) — theory and implementation](https://medium.com/@ferlatti.aldo/neural-style-transfer-nst-theory-and-implementation-c26728cf969d) (Basic NST Blog Post)
 3. [Johnson, Justin and Alahi, Alexandre and Li, Fei-Fei. "Perceptual losses for real-time style transfer and super-resolution." European Conference on Computer Vision. 2016](https://arxiv.org/abs/1603.08155) (Fast NST Paper)
 4. [pytorch/examples/fast_neural_style](https://github.com/pytorch/examples/tree/main/fast_neural_style) (Fast BST Implementation)
 5. [Downsampling and Upsampling of Images — Demystifying the Theory](https://medium.com/analytics-vidhya/downsampling-and-upsampling-of-images-demystifying-the-theory-4ca7e21db24a)