---
title: "From Control Hamiltonian to Algebraic Riccati Equation and Pontryagin's Maximum Principle"
date: 2024-07-16
permalink: /posts/2024/07/16
tags:
  - Algebraic Riccati Equation
  - Linear Qudratic Regulator
  - Calculus of Variations
---
Inspired by the Hamiltonian of classical mechanics, Lev Pontryagin introduced the [_Control Hamiltonian_](https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)) and formulated his celebrated [_Pontragin's Maximum Principle (PMP)_](https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle). This blog will first discuss general cases of optimal control problems, including scenarios with both free final time and final states. Then, the derivation of [_Algebraic Riccati Equation (ARE)_](https://en.wikipedia.org/wiki/Algebraic_Riccati_equation) in the context of **_Continuous Linear Qudratic Regulator (LQR)_** from the perspective of the **_Control Hamiltonian_** will be introduced. It will then explain the **_PMP_**, finally followed by the example problem of **_Constrained Continous LQR_**.

## More general senarios of optimal control problems

In [one of the previous blogs](https://lihanlian.github.io/posts/2024/06/30) on Euler-Lagrange equation, I went through the derivation process for the case of fixed final state and free final time. Now, Let's consider more general cases that involve free final state and free final time. To start with, recall the goal is minimize the objective function as follows:

$$ J[x] = \int_0^{t_f} L(x(t), u(t)) \, dt $$

The first varition of \\(J[x]\\) can be written as:

$$ \delta J[x] = \int_0^{t_f} \left( \frac{\partial L}{\partial x} \delta x + \frac{\partial L}{\partial \dot{x}} \delta \dot{x} \right) dt $$

For the optimal solution, the first varition of the functional \\(J[x]\\) should be zero. Note that in the later section, for the sake of concise notation, I will use \\( L^* \big|_{t_f} \\) to denote \\(L^*(x(t_f), \dot{x}(t_f))\\). Also keep in mind that I will denote $$x(t)$$, $$\dot{x}(t)$$ as $$x$$, $$dot{x}$$ for simplicity too. 
Now consider the most general case, where both final state and final time are free, the first variation can be expressed as:

$$ \delta J^*[x^*] = \int_0^{t_f} \left( \frac{\partial L}{\partial x^*} - \frac{d}{dt} \frac{\partial L}{\partial \dot{x}^*} \right) \delta x \, dt + \int_{t_f}^{t_f + \delta t_f} L^* \big|_{t_f} \, dt + \frac{\partial L}{\partial \dot{x}} \delta x(t_f) $$

Since the perturbation is small, and the resulting change of time interval \\(\delta t_f\\) is also small. As shown in the figure, the following terms can be approximated as:

$$ \int_{t_f}^{t_f + \delta t_f} L^* \big|_{t_f} \, dt \approx L^* \big|_{t_f} \delta t_f$$ 

$$ \delta x_f \approx \delta x(t_f) + \dot{x}^* \delta t_f $$

Thus, $$ \delta J^*[x^*]$$ can be rewritten as:

$$ = \int_0^{t_f} \left( \frac{\partial L}{\partial x} - \frac{d}{dt} \frac{\partial L}{\partial \dot{x}} \right) \delta x \, dt + L^* \big|_{t_f} \delta t_f + \frac{\partial L}{\partial \dot{x}} \left( \delta x_f - \dot{x}^* \delta t_f \right) $$

$$ = \int_0^{t_f} \left( \frac{\partial L}{\partial x} - \frac{d}{dt} \frac{\partial L}{\partial \dot{x}} \right) \delta x \, dt + \frac{\partial L}{\partial \dot{x}} \delta x_f + \left( L^* \big|_{t_f} - \frac{\partial L}{\partial \dot{x}} \dot{x}^*(t_f) \right) \delta t_f $$

<!-- For free final time and fixed final time:

$$
\delta J^*[x^*] = \int_0^{t_f} \left( \frac{\partial L}{\partial x^*} - \frac{d}{dt} \frac{\partial L}{\partial \dot{x}^*} \right) \delta x \, dt + \left[ \frac{\partial L}{\partial \dot{x}^*} \delta x \right]_{t_f}
$$

For free final time: -->
Now we introduce the control input $$u(t)$$ and the terminal cost function $$\Phi(x(t_f), t_f)$$ and Let's derive the expression for the most general case that allow both free final state and free final time. First of all, the optimization problme is formulated as follows:

$$J[x,u] = \int_0^{t_f} L(x(t), u(t)) \, dt \quad \text{s.t.} \quad \dot{x}(t) = f(x(t), u(t)) $$

To handle the equality constraints due to system dynamcis, we introduce the lagrange multiplier $$\lambda$$ (which is also a function of time) and reformulate the objective functional:

$$
J[x,u,\lambda] = \int_0^{t_f} \left[ L(x(t), u(t)) + \lambda^T(t) f(x(t), u(t)) \right] dt + \Phi(x(t_f), t_f)
$$

Define the Lagrangian $$\mathcal{L} = L(x,u) + \lambda^T f(x,u)$$ and the first variation is:

$$
\delta J^*[x,u,\lambda] = \int_0^{t_f} \left[ \left( \nabla_x \mathcal{L}^* - \frac{d}{dt} \nabla_{\dot{x}} \mathcal{L}^* \right) \delta x + \nabla_u \mathcal{L}^* \delta u \right] dt $$

$$
+ \nabla_{\dot{x}} \mathcal{L}^*\big|_tf \delta x_{f} + \left[ \nabla_{\dot{x}} \mathcal{L}^* - \nabla_x \mathcal{L}^*\big|_{t_f}\dot{x}^*(t_f)\right] \delta t_f
$$

$$
+ \nabla_x \Phi^* |_{t_f} \delta x_{f} + \frac{\partial \Phi^*\big|_{t_f}}{\partial t_f} \delta t_f
$$

rearrange the term to separate $$\delta t$$, $$\delta x_{f}$$ and $$\delta t_{f}$$:

$$
\delta J^*[x,u,\lambda] = \int_0^{t_f} \left[ \left( \nabla_x \mathcal{L}^* - \frac{d}{dt} \nabla_{\dot{x}} \mathcal{L}^* \right) \delta x + \nabla_u \mathcal{L}^* \delta u \right] dt $$

$$
+ \left[ \nabla_{\dot{x}} \mathcal{L}^*\big|_tf + \nabla_x \Phi^* |_{t_f}\right]\delta x_{f} 
$$

$$
+ \left[ \nabla_{\dot{x}} \mathcal{L}^* - \nabla_x \mathcal{L}^*\big|_{t_f}\dot{x}^*(t_f) + \frac{\partial \Phi^*\big|_{t_f}}{\partial t_f}\right] \delta t_f 
$$



Consider the following simple robot dynamics (single-integrator model) and the objetive functional aimed to minimize:

$$ J = \frac{1}{2} \int_{0}^{1} (x^2 + u^2) \, dt $$

$$ s.t. \dot{x} = u(t)$$

This is also in [one of the previous blogs](https://lihanlian.github.io/posts/2024/06/30) on Euler-Lagrange Equation, and it's the second example but is a fixed final state and fixed final time problem (\\(x(0) = 0, x(1) = 10,\\)). In this blog, Let's take a look at more general cases including free final state and/or free final time. However, before directly solves the problem, I want to shows the process of derivation common solution strategries for different secaniros using calculus of variations.

**- Fixed final state with free final time**

**- Free final state with fixed final time**

**- Free final state with free final time**

## Continuous LQR and ARE

Introduce the **Control Hamiltonian** to make the expression more compact:

$$
\mathcal{H}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{\lambda}(t)) = g(\mathbf{x}(t), \mathbf{u}(t)) + \mathbf{\lambda}^T(t)[a(\mathbf{x}(t), \mathbf{u}(t))]
$$

$$\dot{x}^* = \nabla_{\lambda}\mathcal{H}^*(x^*(t), u^*(t), \lambda^*(t))$$

$$\dot{\lambda}^* = -\nabla_{x}\mathcal{H}^*(x^*(t), u^*(t), \lambda^*(t))$$

It turns out that the Algebraic Riccati Equation can be derived using PMP.

$$ J = \frac{1}{2} \int_{0}^{T} (x(t)'Qx(t) + u(t)'Ru(t)) \, dt $$
 
$$ s.t. \dot{x} = Ax(t) + Bu(t) $$

### - Formulate Control Hamiltonian:

$$ H(x, u, \lambda, t) = \frac{1}{2} (x'Qx + u'Ru) + \lambda' (Ax + Bu) $$

### - Construsct co-state Equations:

$$ \dot{\lambda} = -\frac{\partial H}{\partial x} $$

$$ \frac{\partial H}{\partial x} = Qx + A'\lambda $$

$$ \dot{\lambda} = -Qx - A'\lambda $$

### - Obtaining Optimal \\( u \\):

Since we are __NOT__ concerned with the control input constraints here, the optimal control input \\(u\\) can be directly obtained by setting the paritial derivatives to be zero.

$$ \frac{\partial H}{\partial u} = Ru + B'\lambda = 0, \quad  u = -R^{-1}B'\lambda$$

### - Substitute optimal \\( u \\) into state space equation:

$$ \dot{x} = Ax + B(-R^{-1}B'\lambda) = Ax - BR^{-1}B'\lambda $$

### - Riccati Equation:

Suppose \\( \lambda = Px \\), then \\( \dot{\lambda} = \dot{P}x + Px \\), substitute into co-state equation: 

$$ \dot{\lambda} = \dot{P}x + P(Ax - BR^{-1}B'Px) = -Qx - A'Px $$

$$ \dot{P}x + PAx - PBR^{-1}B'Px = -Qx - A'Px $$

$$ \dot{P} + PA + A'P - PBR^{-1}B'P + Q = 0 $$

## Pontryagin's Maximum Principle

**- Continuous time PMP**
For a given system an a give optimal criterion, let $$u^* \in \mathcal{U}$$ be <span style="color:red">an</span> optimal control, then there is a variable called costate which together with the state variables satifsfies the Hamilton canonical equations. In addition, it turns out Pontryagin's Maximum Principle is also frequently refered as Pontryagin's Minimum Principle. The difference comes from the different formulasim of the **Control Hamiltonian**. For <span style="color:red">Pontryagin's Maximum Principle</span>, the **Control Hamiltonian (maximum)** is defined as follows:

$$ \mathcal{H}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{\lambda}(t), t) = -L(\mathbf{x}(t), \mathbf{u}(t)) + \mathbf{\lambda}^T(t)[f(\mathbf{x}(t), \mathbf{u}(t))] $$

$$\dot{x}^* = \nabla_{\lambda}\mathcal{H}(x^*(t), u^*(t), \lambda^*(t), t)$$

$$\dot{\lambda}^* = -\nabla_{x}\mathcal{H}(x^*(t), u^*(t), \lambda^*(t), t)$$

$$u^* = \arg \max_{u \in \mathcal{U}} H(x(t)^*, u(t), \lambda(t)^*, t) $$

For <span style="color:red">Pontryagin's Minium Principle</span>, the **Control Hamiltonian (minimum)** is defined as follows:

$$ \mathcal{H}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{\lambda}(t), t) = L(\mathbf{x}(t), \mathbf{u}(t)) + \mathbf{\lambda}^T(t)[f(\mathbf{x}(t), \mathbf{u}(t))] $$

This results in the optimal control is the one that minimize the **Control Hamiltonian**:

$$u^* = \arg \min_{u \in \mathcal{U}} H(x(t)^*, u(t), \lambda(t)^*, t) $$

**- Discrete time PMP and KKT** 

In discrete time setting, the PMP can be viewed as a special case of [Karush–Kuhn–Tucker conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) (KKT), check out the second reference for more details from Prof. Zac Manchester. Let's first take a look at the optimization problem formulated as follows:

$$ \min_{x_k, u_k} \sum_{k=0}^{N-1} L(x_k, u_k) + \Phi(x_N) $$

$$ s.t. \quad x_{k+1} = f(x_k, u_k) $$

Same as before, \\(f(x_k, u_k)\\) defines the system dynamics and \\( L(x_k, u_k) \Phi(x_N)\\) are stage cost and terminal cost function respectively. We can then formulate the Lagrangian and proceed the procedure in the KKT manner:

$$ \mathcal{L} = \sum_{k=0}^{N-1} \left[ L(x_k, u_k) + \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right) \right] + \Phi(x_N) \tag{1} \label{eq:Lagrangian}$$

$$ \frac{\partial \mathcal{L}(x_k, u_k)}{\partial \lambda_{k+1}} = f(x_k, u_k) - x_{k+1} = 0 \quad (k = 0, \cdots ,N-1) \tag{2} \label{eq:state}$$

<!-- - Partial Derivative with respect to \\( x_k (k < N-1 ) \\):  -->

<!-- $$ \frac{\partial L(x_k, u_k)}{\partial x_k} = \left( \frac{\partial f(x_k, u_k)}{\partial x_k} \right)^\top \lambda_{k+1} -  \lambda_k  \quad (n \times 1) $$ -->

$$ \frac{\partial \mathcal{L}}{\partial x_k} = \frac{\partial L(x_k, u_k)}{\partial x_k} + \left( \frac{\partial f(x_k, u_k)}{\partial x_k} \right)^\top \lambda_{k+1} - \lambda_k \\(k = 1, ..., N-1 \\) \tag{3} \label{eq:costate}$$

$$  \frac{\partial \mathcal{L}}{\partial x_N} = \frac{\partial \Phi(x_N)}{\partial x_N} - \lambda_N \quad \\(k = N \\) \tag{4} \label{eq:boundary}$$

To clarify the dimensions of each equations, suppose \\(x\\) has \\(n\\) elements, which is a (\\( n \times 1 \\)) vector at each time step \\(k\\) (and so is $$\lambda$$), and there are \\( k = N\\) time step. At each time step \\(k \\), the partial derivative of Lagrangian 
<!-- Here, the term \( -\lambda_k \) appears because \( x_k \) is also present in the term \( \lambda_k^\top \left( f(x_{k-1}, u_{k-1}) - x_k \right) \) from the previous time step \( k - 1 \). -->

- w.r.t \\( \lambda_{k+1}\\) is a \\( n \times 1 \\) column vector. From \\(k = 0 \cdots N-1\\), results in \\(n \times N\\) equations. Note that we are also given \\(x_0\\), **which is a boundary condition at \\(k = 0 \\) with dimension \\(n \times 1\\)**.

$$
\frac{\partial  \mathcal{L}(x_k, u_k)}{\partial \lambda_{k+1}} = 
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial \lambda_{(k+1)1}} \\
\frac{\partial \mathcal{L}}{\partial \lambda_{(k+1)2}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial \lambda_{(k+1)n}}
\end{bmatrix}
$$

- w.r.t \\( x_k\\) is a \\( n \times 1 \\) column vector. From \\(k = 1, \cdots, N\\), results in \\(n \times (N-1)\\) euqations. 

$$
\frac{\partial  \mathcal{L}(x_k, u_k)}{\partial x_k} = 
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial x_{k1}} \\
\frac{\partial \mathcal{L}}{\partial x_{k2}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial x_{kn}}
\end{bmatrix}
$$

Since \\(x_k\\) is a \\(n \times 1\\) column vector, \\( f(x_k, u_k) \\) is a also a vector, so its Jacobian matrix (\\(n \times n\\))with respect to \\( x_k \\) is given by:

$$
\frac{\partial f(x_k, u_k)}{\partial x_k} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_{k1}} & \frac{\partial f_1}{\partial x_{k2}} & \cdots & \frac{\partial f_1}{\partial x_{kn}} \\
\frac{\partial f_2}{\partial x_{k1}} & \frac{\partial f_2}{\partial x_{k2}} & \cdots & \frac{\partial f_2}{\partial x_{kn}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_{k1}} & \frac{\partial f_n}{\partial x_{k2}} & \cdots & \frac{\partial f_n}{\partial x_{kn}}
\end{bmatrix}
$$

The term \\(-\lambda_k\\) appears because \\(x_k\\) is also present in term \\(\lambda_k^\top \left( f(x_{k-1}, u_{k-1}) - x_{k} \right)\\) from the previous time step \\(k-1\\), thus:

$$
\begin{bmatrix}
\frac{\partial \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right)}{\partial \lambda_{(k+1)1}} \\
\frac{\partial \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right)}{\partial \lambda_{(k+1)2}} \\
\vdots \\
\frac{\partial \lambda_{k+1}^\top \left( f(x_k, u_k) - x_{k+1} \right)}{\partial \lambda_{(k+1)n}}
\end{bmatrix} = 
\frac{\partial  \mathcal{L}(x_k, u_k)}{\partial x_k}^\top \lambda_{k+1}
$$

- w.r.t \\( x_N\\) is a \\( n \times 1 \\) column vector, **which is a boundary condition at \\(k = N \\)**.

$$
\frac{\partial  \mathcal{L}(x_N, u_k)}{\partial x_k} = 
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial x_{N1}} \\
\frac{\partial \mathcal{L}}{\partial x_{N2}} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial x_{Nn}}
\end{bmatrix}
$$

- We can make the above expression more compact by defining the **Control Hamiltonian** (in discrete time):

$$ H_k (x_k, u_k, \lambda_{k+1}) = L(x_k, u_k) + \lambda_{k+1}^\top f(x_k, u_k) $$

The Langrangian function \eqref{eq:Lagrangian} can be then rewritten as:

$$ \mathcal{L} = L(x_0, u_0) + \lambda_{1}^\top f(x_k, u_k) + \sum_{k=1}^{N-1} \left[ L(x_k, u_k) + \lambda_{k+1}^\top f(x_k, u_k) - \lambda_{k}^\top x_{k+1}  \right] + \Phi(x_N) - \lambda_{N}^\top x_N $$

$$ \mathcal{L} = H_0 (x_0, u_0, \lambda_{1}) + \sum_{k=1}^{N-1} \left[ H_k (x_k, u_k, \lambda_{k+1}) - \lambda_{k}^\top x_k  \right] + \Phi(x_N) - \lambda_{N}^\top x_N$$

The stationarity condition requires the gradient of the Lagrangian with respect to both \(x_k\) and \(u_k\) to be zero:

$$ \nabla_{x_k, u_k} \mathcal{L} = 0 $$

The stationarity condition in the KKT framework involves taking the gradient of the Lagrangian with respect to the decision variables (both \\(x_k\\) and \\(u_k\\)) and setting it to zero. This ensures that the Lagrangian is stationary (i.e., no change) at the optimal solution.

In the context of PMP, the stationarity condition with respect to the control variable \(u_k\) is used to derive the optimality condition for the control:

- **State Equation (Primal Feasibility:):** (rearranging \eqref{eq:state})

$$ x_{k+1}^* = \frac{\partial H_k (x_k^*, u_k^*, \lambda_{k+1}^*)}{\partial \lambda^*}$$

- **Costate Equation (Dual Feasibility & Resulting From Stationarity Condition):** (rearranging \eqref{eq:costate})

$$ \lambda_{k}^* = \frac{\partial H_k (x_k^*, u_k^*, \lambda_{k+1}^*)}{\partial x_k^*}$$

Recall that we denote the initial boundary condition at \\( k = 0 \\) by \\( x_0 \\) and the terminal boundary condition at \\( k = N \\) by \\( \lambda_N \\). The boundary conditions at each intermediate point must also be satisfied. For instance, \\( x_1 \\) exists in two equations (\\( x_1 = f(x_0, u_0) \\) and \\( x_2 = f(x_1, u_1) \\)), and it must be consistent and satisfy both equations. Therefore, we have as many equations as boundary conditions to solve the problem. In addition, notice that \\(\lambda_{k+1}^* \\) is on the right and \\( \lambda_{k}^*\\) is on left. This indicates that <span style="color:red"> the costate equation is involving backward in time </span>.

- **Optimality Condition (Resulting From Stationarity Condition):**

In the case of there is no control input constraint, we can just take the partial derivative of the Control Hamiltonian w.r.t. \\(u_k\\) and set is to zero:

$$ \frac{\partial H_k^*}{\partial u_k^*} = 0 $$

In the case of control input constraint exits, we need to find the control input within the admissible set that minimize the Control Hamiltonian:

$$ u_k^* = \arg \min_{u_k \in \mathcal{U}} H_k (x_k^*, u_k^*, \lambda_{k+1}^*) $$

## Example 2: Constrained Cotinuous LQR [1]

Consider the LQR in continuous time setting, the objective function to be minimized is defined as follows:

$$ \min_{x(t), u(t)} \frac{1}{2} \int_{0}^{t_f} \left( x^T(t) Q x(t) + u^T(t) R u(t) \right) dt $$

$$ s.t. \quad \dot{x}(t) = A x(t) + B u(t), $$

$$ \quad x(0) = r_0, \quad x(t_f) = r_f, \quad u \in \mathcal{U} $$

In this case, Let's use the Pontryagin's Maximum Principle, so the **Control Hamiltonian (maximum)** is defined as:

$$ H = - \frac{1}{2} x^T Q x - \frac{1}{2} u^T R u + \lambda^T (A x + B u) $$

The optimal $$u^*$$ in case of no control input constraint can be obtained by 

$$ \nabla_{u} \mathcal{H} = 0 $$

$$ - \frac{1}{2} u^{*T} R u^* + \lambda^{*T} B u^* \geq - \frac{1}{2} u^T R u + \lambda^{*T} B u, \quad u \in \mathcal{U} $$

or

$$ u^* = \arg\max_{u \in \mathcal{U}} \left[ - \frac{1}{2} u^T R u + \lambda^{*T} B u \right] $$

Scalar case (first-ordered and single-input system)

$$ u^*(t) = \arg\max_{u_{\min} \leq u(t) \leq u_{\max}} \left[ - \frac{1}{2} r u(t)^2 + b \lambda^*(t) u(t) \right]$$

Recall \\( r > 0 \\) (bump not valley)

$$
u^* = 
\begin{cases} 
\frac{b \lambda^*}{r} & \text{if } u_{\min} \leq \frac{b \lambda^*}{r} \leq u_{\max} \\
u_{\min} & \text{if } \frac{b \lambda^*}{r} < u_{\min} \\
u_{\max} & \text{if } \frac{b \lambda^*}{r} > u_{\max}
\end{cases}
$$

$$ u^*(t) = \text{sat}_{u_{\min}}^{u_{\max}} \left( \frac{b \lambda^*(t)}{r} \right) $$

Substitute into the Hamilton canonical equations (state and costate equations)

$$ \dot{x} = a x + b \, \text{sat} \left( \frac{b \lambda^*(t)}{r} \right) $$

$$ \dot{\lambda} = - a \lambda + q x $$

In general

$$ u^* \neq \text{sat}(u_{\text{LQR}}) $$



## Summary
 - PMP provides 

### References
 1. [L7.1 Pontryagin's principle of maximum (minimum) and its application to optimal control [YouTube]](https://www.youtube.com/watch?v=Bxc4iy2xUjc&list=PLMLojHoA_QPmRiPotD_TnfdUkglTexuqm&index=16&t=1s) (Explains the difference of Control Hamiltonian formulation in both Maximum and Minimum Principle)
 2. [Optimal Control (CMU 16-745) 2023 Lecture 6: Deterministic Optimal Control Intro [YouTube]](https://www.youtube.com/watch?v=U9zrNwMXktQ&list=PLZnJoM76RM6KugDT9sw5zhAmqKnGeoLRa&index=10) (Deriviation of PMP in discrete time setting, starting at 57 minutes and 15 seconds)
 3. [Karush-Kuhn-Tucker (KKT) conditions: motivation and theorem](https://www.youtube.com/watch?v=K3L7UYnZuZ4&list=PLHAS_3-nESXV6XgW53wSkZHazVE7ZkHAV&index=38) (Part of Intro to Optimization Course by Prof. 
Lewis Mitchell, also contains other great explanation including lagrange multipliers, Netwon's method, etc.)
 4. [Hamiltonian Method of Optimization of Control Systems  [YouTube]](https://www.youtube.com/watch?v=r-fscDKfeUs) (Clear example problem solved using Control Hamiltonian)
 5. [Why the Riccati Equation Is important for LQR Control [YouTube]](https://www.youtube.com/watch?v=ZktL3YjTbB4) (Derivation of ARE from another perspective)
 6. [Matrix Calculus [YouTube]](https://www.youtube.com/watch?v=IgAr5kzza78) (Great explanation on matrix and vector derivatives)
 7. [Geomety of the Pontryagin Maximum Principle  [YouTube]](https://www.youtube.com/watch?v=V04N9X3NxYA&t=9s) (Explanation of PMP from another persepctive)

