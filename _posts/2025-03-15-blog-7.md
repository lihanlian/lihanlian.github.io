---
title: "Shed Some Light on Proximal Policy Optimization (PPO) and Its Application"
date: 2025-05-31
permalink: /posts/blog7
tags:
  - Reinforcement Learning
  - Policy Gradient
  - Proximal Policy Optimization
---
[**_Proximal Policy Optimization (PPO)_**](https://spinningup.openai.com/en/latest/algorithms/ppo.html) is a reinforcement learning algorithm that refines policy gradient methods like **REINFORCE** using **importance sampling** and a clipped surrogate objective to stabilize updates. **PPO-Penalty** explicitly penalizes [**_KL divergence_**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) in the objective function, and **PPO-Clip** instead uses clipping to prevent large policy updates. In many robotics tasks, PPO is first used to train a base policy (potentially with privileged information). Then, a deployable controller is learned from this base policy using **imitation learning**, **distillation**, or other techniques. This blog explores PPO’s core principle, with code available at this <i class="fa-brands fa-github"></i> [repository](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo).


## Policy Gradient

Policy Gradient methods are a foundational class of algorithms in reinforcement learning (RL) that directly optimize a parameterized policy with respect to expected cumulative reward. Unlike **value-based** methods such as Q-learning, **which aim to learn value functions and derive policies indirectly**, policy gradient methods focus on learning the policy itself, *often* represented as a neural network that  <span style="color:red"> **outputs a probability distribution over actions given states** </span>. 

However, policy gradient methods are not without challenges—issues such as **high variance** in gradient estimates and **sample inefficiency** are well-known. To understand the core idea behind this family of methods, we begin with the most basic and historically significant example: **REINFORCE**, a **Monte Carlo-based** algorithm that illustrates how policy gradients can be estimated using sampled trajectories.

- ### REINFORCE

  <figure style="display: block; margin: 0 auto; width: 80%;">
    <img src='/images/blog/blog7/reinforce.png' style="width: 100%;">
    <figcaption style="text-align: center;">REINFORCE algorithms from Wikipedia</figcaption>
  </figure>

  The **REINFORCE** algorithm is an **on-policy** algorithm. By collecting $$ N $$ rollout trajectories, it estimates the policy gradient and optimizes the expected return of a parameterized stochastic policy. $$ \pi_\theta(a \mid s) $$ denotes a policy that defines a distribution over actions given a state, with parameters $$ \theta $$. 
  
  <!-- The objective is to maximize the expected cumulative reward:

  $$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
  $$

  where $$ \tau = (s_0, a_0, s_1, a_1, \ldots) $$ denotes a trajectory sampled from the environment following policy $$ \pi_\theta $$, and $$ R(\tau) $$ is the total return collected along the trajectory. -->

  In **REINFORCE**, the goal is to find a parameterized policy $$\pi_\theta(a \mid s)$$ that maximizes expected return:

  $$
  \theta^* = \arg\max_\theta \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_t r(s_t, a_t) \right]
  $$

  Here, $$\tau = (s_1, a_1, s_2, a_2, \dots, s_T, a_T)$$ is a trajectory sampled from the policy.

  We define the objective function as:

  $$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ r(\tau) \right] = \int \pi_\theta(\tau) r(\tau) \, d\tau
  $$

  - ### Derivation Using the Log Derivative Trick

    Taking the gradient of $$J(\theta)$$:

    $$
    \nabla_\theta J(\theta) = \nabla_\theta \int \pi_\theta(\tau) r(\tau) \, d\tau = \int \nabla_\theta \pi_\theta(\tau) r(\tau) \, d\tau
    $$

    This is problematic because we usually **cannot directly compute** $$\nabla_\theta \pi_\theta(\tau)$$. So we apply the **log-derivative trick**:

    <figure style="display: block; margin: 0 auto; width: 80%;">
      <img src='/images/blog/blog7/log-derivative-trick.png' style="width: 100%;">
      <figcaption style="text-align: center;">Slide from CS 182 Lecture 15</figcaption>
    </figure>

    This identity helps us **convert a gradient of a probability density into something we can sample from**. Substitute it in:

    $$
    \nabla_\theta J(\theta) = \int \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) r(\tau) \, d\tau = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(\tau) r(\tau) \right]
    $$

  - ### Policy Gradient Approximation

    REINFORCE approximates this expectation using Monte Carlo samples:

    $$
    \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \nabla_\theta \log \pi_\theta(\tau^{(i)}) R^{(i)}
    $$

    Where $$R^{(i)} = \sum_t r_t^{(i)}$$ is the return of the $$i$$-th trajectory.

    This is a **Monte Carlo policy gradient estimator**.

 - ### High Variance and the Role of the Baseline

      Despite its theoretical elegance and unbiased gradient estimates, **REINFORCE** suffers from **high variance** and delayed updates, as gradients can only be computed after full trajectory rollouts. These drawbacks motivate subsequent algorithms like **Actor-Critic** and **Proximal Policy Optimization (PPO)**, which improve learning stability through value function baselines and clipped surrogate objectives. 
      
      A common improvement is to subtract a **baseline** $$b(s_t)$$, which does not change the expectation but can reduce variance:

    $$
    \nabla_\theta J(\theta) = \mathbb{E} \left[ \nabla_\theta \log \pi_\theta(a_t \mid s_t) (R_t - b(s_t)) \right]
    $$

    A common choice is to let $$b(s_t) = V^\pi(s_t)$$, the value function under the current policy. This leads naturally to **actor-critic methods**.

- ### Role of Policy Gradient in Modern RL Algorithms

  REINFORCE is a **pure policy gradient** method using only trajectory returns. It's the foundation of more advanced methods:

  | Method                     | Key Idea                                                                 |
  |----------------------------|--------------------------------------------------------------------------|
  | **Actor-Critic**           | Learn both a policy (actor) and value function (critic). The critic estimates $$V(s)$$ as a baseline. |
  | **A2C / A3C**              | Synchronous / asynchronous versions of actor-critic.                     |
  | **GAE (Generalized Advantage Estimation)** | Smoothes the estimation of advantage $$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$ to balance bias-variance. |
  | **TRPO (Trust Region Policy Optimization)** | Adds constraints (KL divergence bounds) to prevent large updates.   |
  | **PPO (Proximal Policy Optimization)**     | Uses importance sampling + clipped objectives to stabilize learning. |

  All of these build on the core idea from REINFORCE: using **gradients of log-probabilities weighted by returns or advantages**.


## KL Divergence and Importance Smapling

In PPO, **KL divergence** measures how much the new policy deviates from the old, ensuring stable updates. **Importance sampling** corrects for using old-policy data to estimate new-policy performance by reweighting with probability ratios, enabling off-policy learning while preserving unbiased gradient estimates. They help with improving learning efficiency and training stability. Let's briefly introduce both of them individually.

- ### Kullback–Leibler (KL) divergence

  **(KL) divergence** is a measure from information theory that quantifies how one probability distribution ($$ P $$) differs from a second, reference probability distribution ($$ Q $$). For two probability distributions $$ P $$ and $$ Q $$ over the same random variable $$ x $$, the KL divergence is defined as:

  $$
  D_{KL}(P \parallel Q) = \sum_x P(x) \log \left( \frac{P(x)}{Q(x)} \right)
  $$

  Or in the continuous case:

  $$
  D_{KL}(P \parallel Q) = \int P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx
  $$

  - It is **not symmetric**: $$ D_{KL}(P \parallel Q) \ne D_{KL}(Q \parallel P) $$
  - It is always **non-negative**, and zero only when $$ P = Q $$ almost everywhere
  - Intuitively, it measures the **"extra bits" needed to represent samples from** $$ P $$ **using a code optimized for** $$ Q $$

  Now, let's walk through a simple concrete example to illustrate the calculation of KL divergence. Suppose we have a random variable $$X$$ that can take on three outcomes:

  $$
  X \in \{A, B, C\}
  $$

  We define two distributions over $$X$$:

  - **True distribution ($$P$$)**: what actually happens  
    $$
    P = \{P(A) = 0.7,\ P(B) = 0.2,\ P(C) = 0.1\}
    $$

  - **Approximate distribution ($$Q$$)**: what we assume or model  
    $$
    Q = \{Q(A) = 0.6,\ Q(B) = 0.3,\ Q(C) = 0.1\}
    $$

  The **KL divergence** $$D_{KL}(P \parallel Q)$$ measures how "surprised" we are when using $$Q$$ to represent $$P$$. Apparently, we need to use the discrete formula for calculation. 

  - For $$x = A$$:

  $$
  0.7 \cdot \log \left( \frac{0.7}{0.6} \right) \approx 0.7 \cdot \log(1.1667) \approx 0.7 \cdot 0.154 \approx 0.1078
  $$

  - For $$x = B$$:

  $$
  0.2 \cdot \log \left( \frac{0.2}{0.3} \right) = 0.2 \cdot \log(0.6667) \approx 0.2 \cdot (-0.4054) \approx -0.0811
  $$

  - For $$x = C$$:

  $$
  0.1 \cdot \log \left( \frac{0.1}{0.1} \right) = 0.1 \cdot \log(1) = 0.1 \cdot 0 = 0
  $$

  Thus,

  $$
  D_{KL}(P \parallel Q) = 0.1078 - 0.0811 + 0 = \boxed{0.0267\ \text{nats}}
  $$

  > **Note**: We're using natural log (ln), so values are in **nats**. For **bits**, use log base 2.

- ### Importance Smapling

  In reinforcement learning, we often evaluate or improve a **new policy** $$\pi$$ while only having data collected from an **old policy** $$\pi_{\text{old}}$$. In this case, **importance sampling** is a critical statistical technique used to **estimate properties of a distribution while only having samples from a different distribution**.

  Let’s say you want to compute the expectation of a function $$f(x)$$ under a distribution $$p(x)$$:

  $$
  \mathbb{E}_{x \sim p}[f(x)] = \int f(x) p(x) \, dx
  $$

  But you only have samples from a **different** distribution $$q(x)$$. You can rewrite the expectation using **importance weights**:

  $$
  \mathbb{E}_{x \sim p}[f(x)] = \int f(x) \frac{p(x)}{q(x)} q(x) \, dx = \mathbb{E}_{x \sim q} \left[ f(x) \frac{p(x)}{q(x)} \right]
  $$

  The term $$\frac{p(x)}{q(x)}$$ is called the **importance weight**, correcting for the fact that samples come from the wrong distribution.

  Typically, after gathering the rollout data (from an old policy), we want to update the neural network parameter more than just one times (gradient descent/ascent) for efficiency. However, each time we perform the gradient updates, we get a different (new) policy and the rollout data distrubtion is still from the old policy. In the case of PPO, this mismatch is handled using importance sampling:

  $$
  \mathbb{E}_{\pi}[\text{reward}] = \mathbb{E}_{\pi_{\text{old}}} \left[ \frac{\pi(a \mid s)}{\pi_{\text{old}}(a \mid s)} \cdot \text{reward} \right]
  $$

## Proximal Policy Optimization (PPO)

- ### Pseudocode

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog7/ppo-pseudocode.png' style="width: 100%;">
  <figcaption style="text-align: center;">PPO Pseudocode from OpenAI Spinning Up</figcaption>
</figure>

- ### Example 1: Cartpole Balancing (<i class="fa-brands fa-github"></i> [cartpole-dqn-ddpg-ppo](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo))

## Combination with Other Methods

- ### Imitation Learning

- ### Example 2: Used with Adaptation Module Learning (<i class="fa-brands fa-github"></i> [hora](https://github.com/HaozhiQi/hora))

## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fab fa-youtube"></i> [REINFORCE: Reinforcement Learning Most Fundamental Algorithm](https://www.youtube.com/watch?v=5eSh5F8gjWU&t=361s) 
 2. <i class="fa-solid fa-blog"></i> [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html) (OpenAI Spinning Up)
 3. <i class="fab fa-youtube"></i> [Overview of Deep Reinforcement Learning Methods](https://www.youtube.com/watch?v=wDVteayWWvU&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=7)
 4. <i class="fab fa-youtube"></i> [CS 182: Lecture 15: Part 1: Policy Gradients](https://www.youtube.com/watch?v=_AYvYUrDohw&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=45) & [Part 2](https://www.youtube.com/watch?v=yiXEXmKhICA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=46) & [Part 3](https://www.youtube.com/watch?v=EsDm2K5nKeA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=47) (Part of CS 182 from UC Berkeley)
 6. <i class="fab fa-youtube"></i> [The KL Divergence : Data Science Basics](https://www.youtube.com/watch?v=q0AkK8aYbLY) & [Importance Sampling](https://www.youtube.com/watch?v=C3p2wI4RAi8)
 5. <i class="fa-brands fa-github"></i> [minimal-isaac-gym](https://github.com/lorenmt/minimal-isaac-gym)