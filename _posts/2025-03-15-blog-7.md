---
title: "Implementation Details of Proximal Policy Optimization (PPO) "
date: 2025-03-15
permalink: /posts/blog7
tags:
  - Reinforcement Learning
  - Policy Gradient
  - Deep Deterministic Policy Gradient
---
[**_Q-learning_**](https://en.wikipedia.org/wiki/Q-learning), an **off-policy** reinforcement learning algorithm, uses the [**_Bellman equation_**](https://en.wikipedia.org/wiki/Bellman_equation) to iteratively update state-action values, helping an agent determine the best actions to maximize cumulative rewards. Deep Q-learning improves upon Q-learning by leveraging deep Q network (DQN) to approximate Q-values, enabling it to handle continuous state spaces but it is still only suitable for **discrete action spaces**. Further advancement, [**_Deep Deterministic Policy Gradient (DDPG)_**](https://arxiv.org/abs/1509.02971), combines Q-learning's principles with **policy gradients**, making it also suitable for continuous action spaces. This blog starts by discussing the basic components of reinforcement learning and gradually explore how Q-learning evolves into DQN and DDPG, with application for solving the cartpole environment in [**_Isaacgym simulator_**](https://github.com/isaac-sim/IsaacGymEnvs). Corresponding code can be found at this <i class="fa-brands fa-github"></i> [repository](https://github.com/lihanlian).

## Reinforcement Learning Framework 

The goal of reinforcement learning is to train an agent to interact with an environment to maximize cumulative rewards over time. Unlike supervised learning, RL does not rely on labeled data but instead learns from feedback in the form of rewards or penalties.

Reinforcement learning problems are typically modeled as **MDPs**. An MDP is defined by the tuple **(S, A, P, R, γ):**

- **S**: Set of states.
- **A**: Set of actions.
- **P(s' | s, a)**: Transition probability to **s'** given **s** and **a**.
- **R(s, a)**: Reward function for taking action **a** in state **s**.
- **γ**: Discount factor to prioritize short-term vs. long-term rewards.

<figure style="display: block; margin: 0 auto; width: 60%;">
  <img src='/images/blog/blog6/rl-framework.png' style="width: 100%;">
  <figcaption style="text-align: center;">source: "What Is Reinforcement Learning?" (Mathwork)</figcaption>
</figure>

- ### Replay Buffer

- ### Target Network


## Deep Q-Learnig

## Deep Deterministic Policy Gradient (DDPG)



## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fa-solid fa-blog"></i> [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html) (OpenAI Spinning Up)
 2. <i class="fab fa-youtube"></i> [Overview of Deep Reinforcement Learning Methods](https://www.youtube.com/watch?v=wDVteayWWvU&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=7)
 3. <i class="fab fa-youtube"></i> [CS 182: Lecture 15: Part 1: Policy Gradients](https://www.youtube.com/watch?v=_AYvYUrDohw&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=45) & [Part 2](https://www.youtube.com/watch?v=yiXEXmKhICA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=46) & [Part 3](https://www.youtube.com/watch?v=EsDm2K5nKeA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=47) (Part of CS 182 from UC Berkeley)
 5. <i class="fa-brands fa-github"></i> [minimal-isaac-gym](https://github.com/lorenmt/minimal-isaac-gym)