---
title: "Shed Some Light on Proximal Policy Optimization (PPO) and Its Application"
date: 2025-04-30
permalink: /posts/blog7
tags:
  - Reinforcement Learning
  - Policy Gradient
  - Proximal Policy Optimization
---
[**_Proximal Policy Optimization (PPO)_**](https://spinningup.openai.com/en/latest/algorithms/ppo.html) is a reinforcement learning algorithm that refines policy gradient methods like **REINFORCE** using **importance sampling** and a clipped surrogate objective to stabilize updates. **PPO-Penalty** explicitly penalizes [**_KL divergence_**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) in the objective function, and **PPO-Clip** instead uses clipping to prevent large policy updates. In many robotics tasks, PPO is first used to train a base policy (potentially with privileged information). Then, a deployable controller is learned from this base policy using **imitation learning**, **distillation**, or other techniques. This blog explores PPOâ€™s core principle, with code available at <i class="fa-brands fa-github"></i> [repo1](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo) and <i class="fa-brands fa-github"></i> [repo2](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo).


## Policy Gradient

Policy Gradient methods are a foundational class of algorithms in reinforcement learning (RL) that directly optimize a parameterized policy with respect to expected cumulative reward. Unlike value-based methods such as Q-learning, which aim to learn value functions and derive policies indirectly, policy gradient methods focus on learning the policy itself, often represented as a neural network that outputs a probability distribution over actions given states. This direct optimization approach is particularly powerful in high-dimensional or continuous action spaces, where traditional methods struggle. By treating the policy as a differentiable function, we can apply gradient ascent techniques to iteratively improve performance through interaction with the environment. 

Policy gradients form the backbone of many modern RL algorithms, including Actor-Critic, DDPG, PPO, TRPO, and SAC, offering flexibility and scalability in complex tasks like robotic control, games, and simulated environments. However, policy gradient methods are not without challengesâ€”issues such as high variance in gradient estimates and sample inefficiency are well-known. To understand the core idea behind this family of methods, we begin with the most basic and historically significant example: REINFORCE, a Monte Carlo-based algorithm that illustrates how policy gradients can be estimated using sampled trajectories.

- ### REINFORCE

  <figure style="display: block; margin: 0 auto; width: 80%;">
    <img src='/images/blog/blog7/reinforce.png' style="width: 100%;">
    <figcaption style="text-align: center;">REINFORCE algorithms from Wikipedia</figcaption>
  </figure>

  The **REINFORCE** algorithm is an **on-policy** algorithm. By collecting $$ N $$ rollout trajectories, it estimates the policy gradient and optimizes the expected return of a parameterized stochastic policy. Let $$ \pi_\theta(a \mid s) $$ denote a policy that defines a distribution over actions given a state, with parameters $$ \theta $$. The objective is to maximize the expected cumulative reward:

  $$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
  $$

  where $$ \tau = (s_0, a_0, s_1, a_1, \ldots) $$ denotes a trajectory sampled from the environment following policy $$ \pi_\theta $$, and $$ R(\tau) $$ is the total return collected along the trajectory.

  Using the log-derivative trick, the gradient of the objective can be expressed as:

  $$
  \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot R_t \right]
  $$

  where $$ R_t = \sum_{\tau=t}^{T} \gamma^{\tau - t} r_\tau $$ is the return from timestep $$ t $$, and $$ \gamma \in [0,1] $$ is the discount factor.

  In practice, this expectation is approximated using $$ N $$ trajectories:

  $$
  g_t \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_{t,n} \mid s_{t,n}) \cdot G_{t,n}
  $$

  where $$ G_{t,n} $$ is the empirical return from time $$ t $$ for the $$ n^\text{th} $$ trajectory. The policy parameters are then updated via gradient ascent:

  $$
  \theta \leftarrow \theta + \alpha \cdot g_t
  $$

  Despite its theoretical elegance and unbiased gradient estimates, REINFORCE suffers from high variance and delayed updates, as gradients can only be computed after full trajectory rollouts. These drawbacks motivate subsequent algorithms like Actor-Critic and Proximal Policy Optimization (PPO), which improve learning stability through value function baselines and clipped surrogate objectives.

- ### Role of Policy Gradient in Modern RL Algorithms

- ### Connection to Optimal Control

## KL Divergence and Importance Smapling

- ### Kullbackâ€“Leibler (KL) divergence

  **(KL) divergence** is a measure from information theory that quantifies how one probability distribution differs from a second, reference probability distribution.

  For two probability distributions $$ P $$ and $$ Q $$ over the same random variable $$ x $$, the KL divergence is defined as:

  $$
  D_{KL}(P \parallel Q) = \sum_x P(x) \log \left( \frac{P(x)}{Q(x)} \right)
  $$

  Or in the continuous case:

  $$
  D_{KL}(P \parallel Q) = \int P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx
  $$

  - It is **not symmetric**: $$ D_{KL}(P \parallel Q) \ne D_{KL}(Q \parallel P) $$
  - It is always **non-negative**, and zero only when $$ P = Q $$ almost everywhere
  - Intuitively, it measures the **"extra bits" needed to represent samples from** $$ P $$ **using a code optimized for** $$ Q $$

  Suppose we have a random variable $$X$$ that can take on three outcomes:

  $$
  X \in \{A, B, C\}
  $$

  We define two distributions over $$X$$:

  - **True distribution ($$P$$)**: what actually happens  
    $$
    P = \{P(A) = 0.7,\ P(B) = 0.2,\ P(C) = 0.1\}
    $$

  - **Approximate distribution ($$Q$$)**: what we assume or model  
    $$
    Q = \{Q(A) = 0.6,\ Q(B) = 0.3,\ Q(C) = 0.1\}
    $$

  Now, letâ€™s compute the **KL divergence** $$D_{KL}(P \parallel Q)$$, which measures how "surprised" we are when using $$Q$$ to represent $$P$$:

  ---

  - ###  â—† KL Divergence Formula (Discrete)

  $$
  D_{KL}(P \parallel Q) = \sum_x P(x) \log \left( \frac{P(x)}{Q(x)} \right)
  $$

  Letâ€™s calculate each term:

  1. For $$x = A$$:

  $$
  0.7 \cdot \log \left( \frac{0.7}{0.6} \right) \approx 0.7 \cdot \log(1.1667) \approx 0.7 \cdot 0.154 \approx 0.1078
  $$

  2. For $$x = B$$:

  $$
  0.2 \cdot \log \left( \frac{0.2}{0.3} \right) = 0.2 \cdot \log(0.6667) \approx 0.2 \cdot (-0.176) \approx -0.0352
  $$

  3. For $$x = C$$:

  $$
  0.1 \cdot \log \left( \frac{0.1}{0.1} \right) = 0.1 \cdot \log(1) = 0.1 \cdot 0 = 0
  $$

  > ðŸ“Œ **Note**: We're using natural log (ln), so values are in **nats**. For **bits**, use log base 2.

  ---

  ### â—† Final Result

  $$
  D_{KL}(P \parallel Q) = 0.1078 - 0.0352 + 0 = \boxed{0.0726\ \text{nats}}
  $$

- ### Importance Smapling

## Proximal Policy Optimization (PPO)

- ### Pseudocode

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog7/ppo-pseudocode.png' style="width: 100%;">
  <figcaption style="text-align: center;">PPO Pseudocode from OpenAI Spinning Up</figcaption>
</figure>

- ### Example 1: Cartpole Balancing (<i class="fa-brands fa-github"></i> [cartpole-dqn-ddpg-ppo](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo))

## Combination with Other Methods

- ### Imitation Learning

- ### Example 2: Used with Adaptation Module Learning (<i class="fa-brands fa-github"></i> [hora](https://github.com/HaozhiQi/hora))

## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamiltonâ€“Jacobiâ€“Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fab fa-youtube"></i> [REINFORCE: Reinforcement Learning Most Fundamental Algorithm](https://www.youtube.com/watch?v=5eSh5F8gjWU&t=361s) 
 2. <i class="fa-solid fa-blog"></i> [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html) (OpenAI Spinning Up)
 3. <i class="fab fa-youtube"></i> [Overview of Deep Reinforcement Learning Methods](https://www.youtube.com/watch?v=wDVteayWWvU&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=7)
 4. <i class="fab fa-youtube"></i> [CS 182: Lecture 15: Part 1: Policy Gradients](https://www.youtube.com/watch?v=_AYvYUrDohw&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=45) & [Part 2](https://www.youtube.com/watch?v=yiXEXmKhICA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=46) & [Part 3](https://www.youtube.com/watch?v=EsDm2K5nKeA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=47) (Part of CS 182 from UC Berkeley)
 6. <i class="fab fa-youtube"></i> [The KL Divergence : Data Science Basics](https://www.youtube.com/watch?v=q0AkK8aYbLY) & [Importance Sampling](https://www.youtube.com/watch?v=C3p2wI4RAi8)
 5. <i class="fa-brands fa-github"></i> [minimal-isaac-gym](https://github.com/lorenmt/minimal-isaac-gym)