---
title: "Shed Some Light on Proximal Policy Optimization (PPO) and Its Application"
date: 2025-04-15
permalink: /posts/blog7
tags:
  - Reinforcement Learning
  - Policy Gradient
  - Proximal Policy Optimization
---
[**_Proximal Policy Optimization (PPO)_**](https://spinningup.openai.com/en/latest/algorithms/ppo.html) is a reinforcement learning algorithm that refines policy gradient methods like **REINFORCE** using **importance sampling** and a clipped surrogate objective to stabilize updates. **PPO-Penalty** explicitly penalizes [**_KL divergence_**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) in the objective function, and **PPO-Clip** instead uses clipping to prevent large policy updates. In many robotics tasks, PPO is first used to train a base policy (potentially with privileged information). Then, a deployable controller is learned from this base policy using **imitation learning**, **distillation**, or other techniques. This blog explores PPO’s core principle, with code available at <i class="fa-brands fa-github"></i> [repo1](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo) and <i class="fa-brands fa-github"></i> [repo2](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo).

## KL Divergence and Importance Smapling

- ### KL Divergence

- ### Importance Smapling


## Policy Gradient

- ### REINFORCE

## Proximal Policy Optimization (PPO)

- ### Pseudocode

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog7/ppo-pseudocode.png' style="width: 100%;">
  <figcaption style="text-align: center;">PPO Pseudocode from OpenAI Spinning Up</figcaption>
</figure>

- ### Example 1: Cartpole Balancing (<i class="fa-brands fa-github"></i> [cartpole-dqn-ddpg-ppo](https://github.com/lihanlian/cartpole-dqn-ddpg-ppo))

## Combination with Other Methods

- ### Imitation Learning

- ### Example 2: Used with Adaptation Module Learning (<i class="fa-brands fa-github"></i> [hora](https://github.com/HaozhiQi/hora))

## Summary
 - One of the most important reason that make **PMP** crucial lies in the fact that maximizing the Hamiltonian is much easier than the original infinite-dimensional control problem. It converts the problem of maximizing over a function space to a pointwise optimization.
 - **ARE** can be viewed be derived from the perspective of **PMP**, whiich is a special case when there is no constraint on control input.
 - In contrast to the **Hamilton–Jacobi–Bellman equation**, which needs to hold over the entire state space to be valid, **PMP** is potentially more computationally efficient in that the conditions which it specifies only need to hold over a particular trajectory.

## References
 1. <i class="fa-solid fa-blog"></i> [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html) (OpenAI Spinning Up)
 2. <i class="fab fa-youtube"></i> [Overview of Deep Reinforcement Learning Methods](https://www.youtube.com/watch?v=wDVteayWWvU&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=7)
 3. <i class="fab fa-youtube"></i> [CS 182: Lecture 15: Part 1: Policy Gradients](https://www.youtube.com/watch?v=_AYvYUrDohw&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=45) & [Part 2](https://www.youtube.com/watch?v=yiXEXmKhICA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=46) & [Part 3](https://www.youtube.com/watch?v=EsDm2K5nKeA&list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&index=47) (Part of CS 182 from UC Berkeley)
 5. <i class="fa-brands fa-github"></i> [minimal-isaac-gym](https://github.com/lorenmt/minimal-isaac-gym)