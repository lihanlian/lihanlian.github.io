---
title: 'On Derivation of Hamilton-Jacobi-Bellman Equation and Its Application'
date: 2024-08-15
permalink: /posts/2024/08/15
tags:
  - Optimal Control
  - Dynamic Programming
  - Reinforcement Learning
---
The [**_Hamilton-Jacobi-Bellman (HJB) equation_**](https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation) is arguably <span style="color:red">one of the most important</span> cornerstones of optimal control theory and reinforcement learning. In this blog, we will first introduce the [**_Hamilton-Jacobi Equation_**](https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi_equation) and [**_Bellman's Principle of Optimality_**](https://en.wikipedia.org/wiki/Bellman_equation). We will then delve into the derivation of the **_HJB equation_**, and highlight some of its key compoents. Finally, we will conclude with an example that shows the derivation of the famous [**_Algebraic Riccati equation (ARE)_**](https://en.wikipedia.org/wiki/Algebraic_Riccati_equation) from this perspective.

## Hamilton-Jacobi Equation

<blockquote class="quote">
    <p>In mathematics, the Hamilton–Jacobi equation is a necessary condition describing extremal geometry in generalizations of problems from the calculus of variations. It can be understood as a special case of the Hamilton–Jacobi–Bellman equation from dynamic programming.</p>
    <footer>&#8211; Wikipedia</footer>
</blockquote>

Introducing the Hamilton-Jacobi equation requires a foundational understanding of Hamiltonian mechanics and the principle of least action. Let's begin by exploring Hamiltonian mechanics, a reformulation of classical mechanics that arises from the Lagrangian mechanics framework. The defintion of the Lagrangian **$$ L $$** can be described as follows:

<figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
    <img src='/images/blog/blog4/lagrangian-definition.png' style="width: 100%;">
    <figcaption style="text-align: center;">Definition of Lagrangian from wikipedia</figcaption>
</figure>

- **Hamiltonian Mechanics**

  - **Hamiltonian ($$ H $$):**

    The Hamiltonian $$ H $$ is defined as:
    
    $$ H(q, p, t) = p \cdot \dot{q} - L(q, \dot{q}, t) $$
    
    where $$ L $$ is the Lagrangian. $$ H $$ often represents the total energy of the system (kinetic + potential energy) but is formulated in terms of coordinates $$ q $$ and momenta $$ p $$.

  - **Generalized Coordinates ($$ q $$) and Conjugate Momenta ($$ p $$):**
    - **$$ q $$** describes the configuration of the system. They can be any variables that uniquely defined the system's state.
    - **$$ p $$**: defined as $$ p = \frac{\partial L}{\partial \dot{q}} $$. This is often interpreted as the momentum associated with each generalized coordinate. It is the key to the transition from Lagrangian to Hamiltonian mechanics.

  - **Hamilton's Equations of Motion:**

    These are first-order differential equations that describe the time evolution of the **$$ q $$** and **$$ p $$**:

    $$ \dot{q} = \frac{\partial H}{\partial p} $$

    $$ \dot{p} = -\frac{\partial H}{\partial q} $$

  - **Phase Space and Canonical Transformations:**

    - **Phase space** is a multidimensional space in which all possible states of a system are represented, with each state corresponding to one unique point in the phase space. For a system with n degrees of freedom, the phase space has 2n dimensions (each degree of freedom contributes one coordinate and one momentum).

    - **Canonical Transformations** are transformations in phase space that preserve the form of Hamilton's equations. They are akin to coordinate transformations in classical mechanics but apply to both coordinates and momenta. Canonical transformations allow for simplifying problems or switching to more convenient variables without changing the physical content of the equations.

- **Principle of Least Action (Hamilton's Principle)**

  - **Action**

    <blockquote class="quote">
      <p>More formally, action is a mathematical functional which takes the trajectory (also called path or history) of the system as its argument and has a real number as its result. Generally, the action takes different values for different paths. Action has dimensions of energy × time or momentum × length, and its SI unit is joule-second (like the Planck constant h).</p>
      <footer>&#8211; Wikipedia</footer>
    </blockquote>

    In short, [action **functional**](https://en.wikipedia.org/wiki/Action_(physics)) returns a scalar quantity descirbes how the balance of the kinetic versus potential energy of a physical system changes with trajectory.

    $$ S[q] = \int_{t_1}^{t_2} L(q, \dot{q}, t) \, dt $$

    where:
    - $$q(t) $$ are the generalized coordinates.
    - $$\dot{q}(t) $$ are the time derivatives of $$q(t)$$ (velocities).
    - $$L(q, \dot{q}, t) $$ is the Lagrangian, typically the difference between kinetic and potential energy.

  - **Hamilton's Principle and Euler-Lagrange Equation**

    [Hamilton's Principle](https://en.wikipedia.org/wiki/Hamilton%27s_principle), also known as the principle of least action, states that the true path taken by a physical system between two points in time is the one that is a stationary point (a point where the variation is zero) of the action functional $$ S[q] $$ (note that $$q$$ is a function of time).

    This means that according to Hamilton's Principle, the actual path taken by the system makes the $$ S[q(t)]$$ stationary (usually a minimum). In terms of functional analysis, this implies the true evolution a phsical system is the solution of the following functional equation:

    $$
      \begin{array}{|c|}
      \hline
      \textbf{Hamilton's principle} \\
      \frac{\delta S}{\delta q(t)} = 0. \\
      \hline
      \end{array}
    $$
    
    This basically says that the first varition of the functional $$ S $$ is zero, which naturally relates to the Euler-Lagrange equations that I haved mentioned in one of the other [blogs post](https://lihanlian.github.io/posts/2024/06/30):

    <figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
      <img src='/images/blog/blog4/euler-lagrange-equation.png' style="width: 100%;">
      <figcaption style="text-align: center;">Statement of Euler-Lagrange Equation from Wikipedia</figcaption>
    </figure>
  
  - **Genrating Function S(q,t)**

    The generating <span style="color:red">**function**</span> (sometimes called Hamilton's Principal function) $$ S(q, t) $$ is used in canonical transformations to change the coordinates in a way that preserves the form of Hamilton's equations. The purpose of using $$ S(q,t) $$ is to simplify the Hamiltonian or to transform the problem into a more tractable form. It effectively changes both the coordinates $$ q $$ and $$ p $$.

    $$S(q, t)$$ is expressed as an integral of the Lagrangian, but it represents the action evaluated along the optimal path, from the initial time $$t_1$$ to time $$t_2$$.

    $$ S(q, t) = \int_{t_1}^{t_2} L(q(\tau), \dot{q}(\tau), \tau) \, d\tau $$

      - $$S(q, t)$$ is a <span style="color:red">**function**</span> of $$q$$ and $$t$$, because it gives a scalar value for specific values of $$q$$ and $$t$$.
        
      - The action functional $$S[q(t)]$$ is different. It is a <span style="color:red">**functional**</span> because it assigns a real number (the action) to a function $$q(t)$$, i.e., the entire trajectory.

    Thus, the generating function $$S(q, t)$$ corresponds to the trajectory $$q(t)$$ for which the first variation $$\delta S[q]$$ of the action functional is zero, meaning that the action is stationary (usually minimized). The Hamilton-Jacobi equation is a way to express this optimal path directly in terms of the generating function $$S(q, t)$$.
       
- **Derivation of HJ Equation**

  <figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
    <img src='/images/blog/blog4/HJ-overview.png' style="width: 100%;">
    <figcaption style="text-align: center;">Overview of Hamilton-Jacobi Equation from Wikipedia</figcaption>
  </figure>  

  After introduction the concept of Hamiltonian mechanics and principle of least action, Let's now elaborate on the steps to derive the Hamilton-Jacobi equation:

  1. **Action and Hamilton's Principal Function:**
    - The action \( S \) is given by:
      $$
      S = \int_{t_1}^{t_2} L(q, \dot{q}, t) \, d\tau
      $$
    - This $$ S(q, t) $$ is Hamilton's principal function, which satisfies the stationary action principle.

  2. **Hamiltonian Formulation:**
    - From the Lagrangian $$ L $$, the Hamiltonian $$ H $$ is defined as:
      $$
      H(q, p, t) = p \cdot \dot{q} - L(q, \dot{q}, t)
      $$
    - Here, $$ p = \frac{\partial L}{\partial \dot{q}} $$ are the canonical momenta.

  3. **Canonical Transformation:**
    - Hamilton's principal function $$ S(q, t) $$ serves as a generating function. This transforms the original coordinates $$(q, p)$$ to new ones where the Hamiltonian may take a simplified form.

  4. **Hamilton's Equations:**
    - The equations of motion are:
      $$
      \dot{q} = \frac{\partial H}{\partial p}
      $$
      $$
      \dot{p} = -\frac{\partial H}{\partial q}
      $$

  5. **Total Differential of $$ S $$:**
    - The total differential of $$ S $$ is:
      $$
      dS = \frac{\partial S}{\partial q} dq + \frac{\partial S}{\partial t} dt
      $$
    - Recognizing that $$ \frac{\partial S}{\partial q} = p $$, we substitute this into the Hamiltonian.

  6. **Deriving the Hamilton-Jacobi Equation:**
    - Substitute $$ p = \frac{\partial S}{\partial q} $$ into the Hamiltonian $$ H $$:
      $$
      H \left( q, \frac{\partial S}{\partial q}, t \right) + \frac{\partial S}{\partial t} = 0
      $$
    - This results in the Hamilton-Jacobi equation:
      $$
      H \left( q, \frac{\partial S}{\partial q}, t \right) + \frac{\partial S}{\partial t} = 0
      $$

    The relationship:

    $$ p = \frac{\partial S}{\partial q} $$

    indicates that the new momenta $$p$$ are derived from the gradient of the action with respect to the new coordinates $$q$$.


## Example 1: Derivation of Newton's Second Law

## Bellman's Principle of Optimality

 - **Discrete time Bellman Equation**

    Bellman's principle of optimality states that an optimal policy has the property that, regardless of the initial state and decision, the remaining decisions must form an optimal policy concerning the state resulting from the first decision.
    $$
    V(x_t) = \min_{u_t} \{ g(x_t, u_t) + \beta V(x_{t+1}) \}
    $$
    where:
    - $$ V(x_t) $$ is the value function representing the minimum cost-to-go from state $$ x_t $$.
    - $$ g(x_t, u_t) $$ is the immediate cost incurred at time $$ t $$.
    - $$ \beta $$ is the discount factor.
    - $$ x_{t+1} $$ is the state at the next time step resulting from applying control $$ u_t $$ to state $$ x_t $$.


 - **Continuous time Bellman Equation (Derivation of HJB equation)**

  $$
  \frac{d}{dt} \mathbf{x} = \mathbf{f}(\mathbf{x}(t), \mathbf{u}(t), t)
  $$

  $$
  J(\mathbf{x}(t), \mathbf{u}(t), t_0, t_f) = Q(\mathbf{x}(t_f), t_f) + \int_{t_0}^{t_f} \mathcal{L}(\mathbf{x}(\tau), \mathbf{u}(\tau)) d\tau
  $$

  $$
  V(\mathbf{x}(t_0), t_0, t_f) = \min_{\mathbf{u}(t)} J(\mathbf{x}(t), \mathbf{u}(t), t_0, t_f)
  $$

  $$
  \frac{\partial V}{\partial t} = \min_{\mathbf{u}(t)} \left( \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \mathbf{f}(\mathbf{x}, \mathbf{u}(t)) + \mathcal{L}(\mathbf{x}, \mathbf{u}) \right)
  $$

  $$
  \frac{d}{dt} V(\mathbf{x}(t), t, t_f) = \frac{\partial V}{\partial t} + \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \frac{d \mathbf{x}}{dt}
  $$

  $$
  = \min_{\mathbf{u}(t)} \frac{d}{dt} \left( \int_{0}^{t_f} \mathcal{L}(\mathbf{x}(\tau), \mathbf{u}(\tau)) d\tau + Q(\mathbf{x}(t_f), t_f) \right)
  $$

  $$
  = \min_{\mathbf{u}(t)} \frac{d}{dt} \left( \int_{0}^{t_f} \mathcal{L}(\mathbf{x}(\tau), \mathbf{u}(\tau)) d\tau \right)
  $$

  $$
  = \min_{\mathbf{u}(t)} \left( \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \mathbf{f}(\mathbf{x}, \mathbf{u}) + \mathcal{L}(\mathbf{x}, \mathbf{u}) \right)
  $$

  $$
  -\frac{\partial V}{\partial t} = \min_{\mathbf{u}(t)} \left( \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \mathbf{f}(\mathbf{x}, \mathbf{u}) + \mathcal{L}(\mathbf{x}, \mathbf{u}) \right)
  $$

## Example 2: Derivation of ARE using HJB equation

## Key points of HJB equations

- **Hamiltonian in HJ and HJB equations**

- **Solving the value function V(x)**

- **Comparison between PMP and HJB**





## Summary
 - Function vs. Functional: While the action $$S[q(t)]$$ is a functional that depends on the entire trajectory, $$S(q, t)$$ is a function representing the minimized action at a specific point and time.

### References
 - [Nonlinear Control: Hamilton Jacobi Bellman (HJB) and Dynamic Programming [YouTube]](https://www.youtube.com/watch?v=-hO-AnFYm6M&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=8&t=929s) (Part of excellent series on optimal control and reinforcement learning by Prof. Steve Brunton.)
 - [EE 564: Lecture 26 (Optimal Control): The Hamilton Jacobi Bellman Approach [YouTube]](https://www.youtube.com/watch?v=kDtcg6U49kY&t=1s) (Great lecture that include the derivation of ARE using HJB and connection with PMP.)
 - [Explaining the Principle of Least Action: Physics Mini Lesson [YouTube]](https://www.youtube.com/watch?v=sUk9y23FPHk) (Include derivation of Newton's Second Law.)
 - [The principle of least action [YouTube]](https://www.youtube.com/watch?v=xz7jLnWcxMs)
 - [Hamiltonian Mechanics in 10 Minutes [YouTube]](https://www.youtube.com/watch?v=B6PCntP3cek) and [Understanding Hamiltonian mechanics: (1) The math [YouTube]](https://www.youtube.com/watch?v=FGQddvjP19w&list=PLmNMSMaNjnDd9Qj4VxNL8dijiWZCAzanl) (More in-depth explanation of Hamiltonian mechanics.)
 - [Action principle: geometric and physical interpretation [YouTube]](https://www.youtube.com/watch?v=7M0BzJhw4wA)