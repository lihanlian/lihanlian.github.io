---
title: 'On Derivation of Hamilton-Jacobi-Bellman Equation and Its Application'
date: 2024-08-15
permalink: /posts/2024/08/15
tags:
  - Optimal Control
  - Dynamic Programming
  - Reinforcement Learning
---
The [**_Hamilton-Jacobi-Bellman (HJB) equation_**](https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation) is arguably <span style="color:red">one of the most important</span> cornerstones of optimal control theory and reinforcement learning. In this blog, we will first introduce the [**_Hamilton-Jacobi Equation_**](https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi_equation) and [**_Bellman's Principle of Optimality_**](https://en.wikipedia.org/wiki/Bellman_equation). We will then delve into the derivation of the **_HJB equation_**, and highlight some of its key compoents. Finally, we will conclude with an example that shows the derivation of the famous [**_Algebraic Riccati equation (ARE)_**](https://en.wikipedia.org/wiki/Algebraic_Riccati_equation) from this perspective.

## Hamilton-Jacobi Equation

<blockquote class="quote">
    <p>In mathematics, the Hamilton–Jacobi equation is a necessary condition describing extremal geometry in generalizations of problems from the calculus of variations. It can be understood as a special case of the Hamilton–Jacobi–Bellman equation from dynamic programming.</p>
    <footer>&#8211; Wikipedia</footer>
</blockquote>

Introducing the Hamilton-Jacobi equation requires a foundational understanding of Hamiltonian mechanics and the principle of least action. Let's begin by exploring Hamiltonian mechanics, a reformulation of classical mechanics that arises from the Lagrangian mechanics framework. The defintion of the Lagrangian **$$ L $$** can be described as follows:

<figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
    <img src='/images/blog/blog4/lagrangian-definition.png' style="width: 100%;">
    <figcaption style="text-align: center;">Definition of Lagrangian from wikipedia</figcaption>
</figure>

- **Hamiltonian Mechanics**

  - **Hamiltonian ($$ H $$):**

    The Hamiltonian $$ H $$ is defined as:
    
    $$ H(q, p, t) = p \cdot \dot{q} - L(q, \dot{q}, t) $$
    
    where $$ L $$ is the Lagrangian. $$ H $$ often represents the total energy of the system (kinetic + potential energy) but is formulated in terms of coordinates $$ q $$ and momenta $$ p $$.

  - **Generalized Coordinates ($$ q $$) and Conjugate Momenta ($$ p $$):**
    - **$$ q $$** describes the configuration of the system. They can be any variables that uniquely defined the system's state.
    - **$$ p $$**: defined as $$ p = \frac{\partial L}{\partial \dot{q}} $$. This is often interpreted as the momentum associated with each generalized coordinate. It is the key to the transition from Lagrangian to Hamiltonian mechanics.

  - **Hamilton's Equations of Motion:**

    These are first-order differential equations that describe the time evolution of the **$$ q $$** and **$$ p $$**:

    $$ \dot{q} = \frac{\partial H}{\partial p} $$

    $$ \dot{p} = -\frac{\partial H}{\partial q} $$

  - **Phase Space and Canonical Transformations:**

    - **Phase space** is a multidimensional space in which all possible states of a system are represented, with each state corresponding to one unique point in the phase space. For a system with n degrees of freedom, the phase space has 2n dimensions (each degree of freedom contributes one coordinate and one momentum).

    - **Canonical Transformations** are transformations in phase space that preserve the form of Hamilton's equations. They are akin to coordinate transformations in classical mechanics but apply to both coordinates and momenta. Canonical transformations allow for simplifying problems or switching to more convenient variables without changing the physical content of the equations.

- **Principle of Least Action (Hamilton's Principl)**

  - **Action**

    Hamilton's Principle, also known as the principle of least action, states that the true path taken by a physical system between two points in time is the one that minimizes the action functional $$ S[q] $$:

    $$ S[q] = \int_{t_1}^{t_2} L(q, \dot{q}, t) \, dt $$

    Here:
    - $$q(t) $$ are the generalized coordinates.
    - $$\dot{q}(t) $$ are the time derivatives of $$q(t)$$ (velocities).
    - $$L(q, \dot{q}, t) $$ is the Lagrangian, typically the difference between kinetic and potential energy.

    The [action](https://en.wikipedia.org/wiki/Action_(physics)) $$ S $$ is defined as the integral of the Lagrangian $$L $$ over time. According to Hamilton's Principle, the actual path taken by the system makes the action $$S $$ stationary (usually a minimum). This leads to the Euler-Lagrange equations:

    $$ \frac{d}{dt} \left( \frac{\partial L}{\partial \dot{q}} \right) - \frac{\partial L}{\partial q} = 0 $$

    <figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
      <img src='/images/blog/blog4/euler-lagrange-equation.png' style="width: 100%;">
      <figcaption style="text-align: center;">Statement of Euler-Lagrange Equation from Wikipedia</figcaption>
    </figure>
  
  - **Genrating Function**

    The function $$S(q, t)$$ can be seen as transforming the original coordinates and momenta to new ones. Specifically:

    - $$q \rightarrow q'$$
    - $$p \rightarrow p'$$

    The transformation ensures that the new coordinates still satisfy Hamilton's equations, but potentially with a simplified Hamiltonian. The relationship:

    $$ p = \frac{\partial S}{\partial q} $$

    indicates that the new momenta $$p$$ are derived from the gradient of the action with respect to the new coordinates $$q$$.

    
- **Derivation of HJ Equation**

  Let's elaborate on the steps to derive the Hamilton-Jacobi equation, involving Hamilton's Principle and the Hamiltonian framework:

  1. **Action and Hamilton's Principal Function:**
    - The action \( S \) is given by:
      $$
      S = \int_{t_1}^{t_2} L(q, \dot{q}, t) \, d\tau
      $$
    - This $$ S(q, t) $$ is Hamilton's principal function, which satisfies the stationary action principle.

  2. **Hamiltonian Formulation:**
    - From the Lagrangian $$ L $$, the Hamiltonian $$ H $$ is defined as:
      $$
      H(q, p, t) = p \cdot \dot{q} - L(q, \dot{q}, t)
      $$
    - Here, $$ p = \frac{\partial L}{\partial \dot{q}} $$ are the canonical momenta.

  3. **Canonical Transformation:**
    - Hamilton's principal function $$ S(q, t) $$ serves as a generating function. This transforms the original coordinates $$(q, p)$$ to new ones where the Hamiltonian may take a simplified form.

  4. **Hamilton's Equations:**
    - The equations of motion are:
      $$
      \dot{q} = \frac{\partial H}{\partial p}
      $$
      $$
      \dot{p} = -\frac{\partial H}{\partial q}
      $$

  5. **Total Differential of $$ S $$:**
    - The total differential of $$ S $$ is:
      $$
      dS = \frac{\partial S}{\partial q} dq + \frac{\partial S}{\partial t} dt
      $$
    - Recognizing that $$ \frac{\partial S}{\partial q} = p $$, we substitute this into the Hamiltonian.

  6. **Deriving the Hamilton-Jacobi Equation:**
    - Substitute $$ p = \frac{\partial S}{\partial q} $$ into the Hamiltonian $$ H $$:
      $$
      H \left( q, \frac{\partial S}{\partial q}, t \right) + \frac{\partial S}{\partial t} = 0
      $$
    - This results in the Hamilton-Jacobi equation:
      $$
      H \left( q, \frac{\partial S}{\partial q}, t \right) + \frac{\partial S}{\partial t} = 0
      $$


## Example 1: Derivation of Newton's Second Law

## Bellman's Principle of Optimality

 - **Discrete time Bellman Equation**

    Bellman's principle of optimality states that an optimal policy has the property that, regardless of the initial state and decision, the remaining decisions must form an optimal policy concerning the state resulting from the first decision.
    $$
    V(x_t) = \min_{u_t} \{ g(x_t, u_t) + \beta V(x_{t+1}) \}
    $$
    where:
    - $$ V(x_t) $$ is the value function representing the minimum cost-to-go from state $$ x_t $$.
    - $$ g(x_t, u_t) $$ is the immediate cost incurred at time $$ t $$.
    - $$ \beta $$ is the discount factor.
    - $$ x_{t+1} $$ is the state at the next time step resulting from applying control $$ u_t $$ to state $$ x_t $$.


 - **Continuous time Bellman Equation (Derivation of HJB equation)**

  $$
  \frac{d}{dt} \mathbf{x} = \mathbf{f}(\mathbf{x}(t), \mathbf{u}(t), t)
  $$

  $$
  J(\mathbf{x}(t), \mathbf{u}(t), t_0, t_f) = Q(\mathbf{x}(t_f), t_f) + \int_{t_0}^{t_f} \mathcal{L}(\mathbf{x}(\tau), \mathbf{u}(\tau)) d\tau
  $$

  $$
  V(\mathbf{x}(t_0), t_0, t_f) = \min_{\mathbf{u}(t)} J(\mathbf{x}(t), \mathbf{u}(t), t_0, t_f)
  $$

  $$
  \frac{\partial V}{\partial t} = \min_{\mathbf{u}(t)} \left( \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \mathbf{f}(\mathbf{x}, \mathbf{u}(t)) + \mathcal{L}(\mathbf{x}, \mathbf{u}) \right)
  $$

  $$
  \frac{d}{dt} V(\mathbf{x}(t), t, t_f) = \frac{\partial V}{\partial t} + \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \frac{d \mathbf{x}}{dt}
  $$

  $$
  = \min_{\mathbf{u}(t)} \frac{d}{dt} \left( \int_{0}^{t_f} \mathcal{L}(\mathbf{x}(\tau), \mathbf{u}(\tau)) d\tau + Q(\mathbf{x}(t_f), t_f) \right)
  $$

  $$
  = \min_{\mathbf{u}(t)} \frac{d}{dt} \left( \int_{0}^{t_f} \mathcal{L}(\mathbf{x}(\tau), \mathbf{u}(\tau)) d\tau \right)
  $$

  $$
  = \min_{\mathbf{u}(t)} \left( \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \mathbf{f}(\mathbf{x}, \mathbf{u}) + \mathcal{L}(\mathbf{x}, \mathbf{u}) \right)
  $$

  $$
  -\frac{\partial V}{\partial t} = \min_{\mathbf{u}(t)} \left( \left( \frac{\partial V}{\partial \mathbf{x}} \right)^T \mathbf{f}(\mathbf{x}, \mathbf{u}) + \mathcal{L}(\mathbf{x}, \mathbf{u}) \right)
  $$

## Example 2: Derivation of ARE using HJB equation

## Key points of HJB equations

- **Hamiltonian in HJ and HJB equations**

- **Solving the value function V(x)**

- **Comparison between PMP and HJB**





## Summary

### References
 - [Nonlinear Control: Hamilton Jacobi Bellman (HJB) and Dynamic Programming [YouTube]](https://www.youtube.com/watch?v=-hO-AnFYm6M&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&index=8&t=929s) (Part of excellent series on optimal control and reinforcement learning by Prof. Steve Brunton.)
 - [EE 564: Lecture 26 (Optimal Control): The Hamilton Jacobi Bellman Approach [YouTube]](https://www.youtube.com/watch?v=kDtcg6U49kY&t=1s) (Great lecture that include the derivation of ARE using HJB and connection with PMP.)
 - [Explaining the Principle of Least Action: Physics Mini Lesson [YouTube]](https://www.youtube.com/watch?v=sUk9y23FPHk) (Include derivation of Newton's Second Law.)
 - [The principle of least action [YouTube]](https://www.youtube.com/watch?v=xz7jLnWcxMs)
 - [Hamiltonian Mechanics in 10 Minutes [YouTube]](https://www.youtube.com/watch?v=B6PCntP3cek) and [Understanding Hamiltonian mechanics: (1) The math [YouTube]](https://www.youtube.com/watch?v=FGQddvjP19w&list=PLmNMSMaNjnDd9Qj4VxNL8dijiWZCAzanl) (More in-depth explanation of Hamiltonian mechanics.)
 - [Action principle: geometric and physical interpretation [YouTube]](https://www.youtube.com/watch?v=7M0BzJhw4wA)