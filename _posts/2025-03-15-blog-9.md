---
title: "Implementation Details of TD3-SAC-Gymnasium"
date: 2025-12-15
permalink: /posts/blog9
tags:
  - Reinforcement Learning
  - Soft Acotr-Critic
  - Entropy
---
**Inverse Kinematics (IK)** converts a desired end-effector pose into joint angles and is fundamental for basic motion planning. **Operational Space Control (OSC)** layers dynamics on top of **IK**, commanding Cartesian forces/velocities so the robot can track trajectories while rejecting disturbances. **Impedance Control** then shapes the robotâ€™s apparent mass-spring-damper behavior, enabling compliant contact and human-robot interaction. [**Riemannian Motion Policy (RMP)**](https://arxiv.org/abs/1801.02854) generalizes impedance ideas across multiple task maps, blending goal-reaching, joint limits, and obstacle avoidance on curved manifolds. Finally, [**Geometric Fabrics**](https://arxiv.org/abs/2109.10443) build on **RMP** to sculpt global energy landscapes, yielding provably safe navigation around complex environments. This post aims to provide brief explanation for each concepts.

## RL Categories

- ### Value-based, Policy-based, and Actor-Critic

- ### On-policy vs Off-policy


## Twin Delayed Deep Deterministic Policy Gradient (TD3)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/td3-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">TD3 Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Results

## Entropy


- ### Tanh Normalization

## Soft Actor-Critic (SAC)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/sac-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">SAC Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Gradient Flow (with torch.no_grad(), etc.)

## References
 1. <i class="fa-solid fa-book-open"></i> [Addressing Function Approximation Error in Actor-Critic Methodsl](https://proceedings.mlr.press/v80/fujimoto18a) (TD3 Paper)
 2. <i class="fa-solid fa-book-open"></i> [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://proceedings.mlr.press/v80/haarnoja18b) (SAC Paper)
 3. <i class="fa-solid fa-blog"></i> [TD3](https://spinningup.openai.com/en/latest/algorithms/td3.html), [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html) (OpenAI Spinning Up) 
 4. <i class="fab fa-youtube"></i> [Entropy Clearly Explained!!!](https://www.youtube.com/watch?v=YtebGVx-Fxw) & [Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM&t=17s)
 5. <i class="fa-brands fa-github"></i> [mjctrl](https://github.com/kevinzakka/mjctrl) & [RMP2](https://github.com/UWRobotLearning/rmp2)
