---
title: "Implementation Details of TD3-SAC-Gymnasium"
date: 2025-12-15
permalink: /posts/blog9
tags:
  - Reinforcement Learning
  - Soft Acotr-Critic
  - Entropy
---
**Twin Delayed Deep Deterministic Policy Gradient (TD3)** and **Soft Actor-Critic (SAC)** are off-policy actor-critic algorithms designed for continuous control tasks where classic **DDPG** can be unhandy. [**TD3**](https://proceedings.mlr.press/v80/fujimoto18a) stabilizes learning with tricks such as double Q-networks, delayed policy updates, and target policy smoothing to reduce overestimation bias. [**SAC**](https://proceedings.mlr.press/v80/haarnoja18b) instead learns a stochastic policy by maximizing both task reward and **entropy**, encouraging robust and exploratory behaviors. This blog post explains core components and implementation details of both algorithms. Corresponding **PyTorch** implementation can be found at this <i class="fa-brands fa-github"></i> [repository](https://github.com/lihanlian/td3-sac-gymnasium).

## Categories of RL Algorithms
Before diving into TD3 and SAC, it is helpful to place them within the broader landscape of reinforcement learning (RL). In this section we briefly review two common ways of categorizing RL algorithms: (i) how they represent and optimize behavior (value-based, policy-based, or actor–critic), and (ii) how they use experience (on-policy vs off-policy). This perspective will make it clearer why TD3 and SAC are usually described as **off-policy** [**actor–critic algorithms**](https://en.wikipedia.org/wiki/Actor-critic_algorithm) for continuous control.

- **Value-based vs Policy-based vs Actor–Critic**

  - **Value-based methods.**  
    Value-based algorithms learn a value function, such as a state–action value $$Q(s,a)$$ or state value $$V(s)$$, and derive a policy by acting greedily with respect to this estimate. For example, a greedy policy is  
    
    $$
      \pi(s) = \arg\max_a Q(s,a).
    $$  

    Classic examples include tabular Q-learning and deep Q-networks (DQN). The policy is *implicit* in the value function; there is usually no separate set of policy parameters.

  - **Policy-based methods (pure policy gradient).**  
    Pure policy-gradient methods parameterize the policy directly as $$\pi_\theta(a \mid s)$$ and optimize the expected return
    
    $$
      J(\theta) = \mathbb{E}_{\pi_\theta}\!\left[\sum_{t=0}^{\infty} \gamma^t r_t \right]
    $$

    via gradients of the form
    
    $$
      \nabla_\theta J(\theta)
      = \mathbb{E}_{\pi_\theta}\!\big[ \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, \hat{G}_t \big],
    $$

    where $$\hat{G}_t$$ is some return estimate. The simplest example is REINFORCE, which can work *without* a learned value function by using Monte Carlo returns as $$\hat{G}_t$$.

  - **Actor–critic methods (policy gradient with a critic).**  
    Actor–critic algorithms combine both ideas. An **actor** $$\pi_\theta(a \mid s)$$ chooses actions, while a **critic** estimates a value function (e.g., $$V_\phi(s)$$, $$Q_\phi(s,a)$$, or an advantage $$A_\phi(s,a)$$) to provide low-variance learning signals for the actor:
    
    $$
      \nabla_\theta J(\theta)
      \approx \mathbb{E}\!\big[ \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, \hat{A}_\phi(s_t,a_t) \big].
    $$

    Algorithms such as A2C/A3C, TRPO (in its common actor–critic form), PPO, TD3, and SAC fall into this category: they use a policy-gradient objective *together with* an explicit value function learned by the critic.

- **On-policy vs Off-policy**

  To explain this distinction, it is useful to separate two roles a policy can play:

  - **Behavior policy** $$\mu(a \mid s)$$: the policy that actually acts in the environment to generate data $$(s_t, a_t, r_t, s_{t+1})$$.
  - **Target policy** $$\pi(a \mid s)$$: the policy we are trying to evaluate or improve in our update rule.

  - **On-policy algorithms**  
    On-policy methods learn about a target policy using data collected from (essentially) the **same** policy:
    $$
      \mu(a \mid s) \approx \pi(a \mid s).
    $$
    In practice, the behavior policy might add a bit of exploration noise, or we might update $$\pi$$ slightly during training, but <span style="color:red"> the algorithm is designed so that the data always come from a policy very close to the one being optimized </span>.  
    Examples:
    - SARSA learns the value of the current $$\epsilon$$-greedy policy.  
    - PPO collects rollouts with an old policy $$\pi_{\text{old}}$$ and then updates $$\pi_\theta$$ while constraining it (via clipping or a KL penalty) so that $$\pi_\theta \approx \pi_{\text{old}}$$. Old data are only used for a few epochs and then discarded.

  - **Off-policy algorithms**  
    Off-policy methods allow the behavior and target policies to be **different**:
    $$
      \mu(a \mid s) \neq \pi(a \mid s).
    $$
    <span style="color:red"> This makes it possible to reuse past experience, learn from old versions of the policy, or even learn from demonstrations generated by some other agent </span>.  
    Examples:
    - Q-learning and DQN typically use an $$\epsilon$$-greedy behavior policy $$\mu$$, but update towards the greedy policy
      $$
        \pi_{\text{greedy}}(s) = \arg\max_a Q(s,a),
      $$
      so $$\mu \neq \pi_{\text{greedy}}$$.  
    - DDPG, TD3, and SAC store transitions in a replay buffer and train the current actor–critic using data that were collected many steps ago under different (more exploratory) policies. This clear separation between data collection and policy improvement is what makes them off-policy.

In the following sections we will see that TD3 and SAC sit in the intersection of these categories: they are **off-policy actor–critic** algorithms that learn a critic $$Q_\phi(s,a)$$ and a continuous-control actor $$\pi_\theta(a \mid s)$$ using replayed experience.


## Twin Delayed Deep Deterministic Policy Gradient (TD3)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/td3-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">TD3 Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Results

## Stochastic Policy
In stochastic policies, the agent samples actions from a state-conditioned distribution $$\pi_\theta(\cdot\mid s)$$ to enable exploration and define a valid density for entropy regularization. Soft Actor-Critic (SAC) is a stochastic, entropy-regularized actor–critic that maximizes value while encouraging high-entropy behavior. Practically, SAC samples from an unbounded Gaussian and applies a smooth $$\tanh$$ squashing to satisfy action bounds; the change-of-variables rule yields a correct log-probability, so both gradients and entropy are well defined.

- ### Gradient Estimators
  We seek gradients of an expectation where the sampled action depends on parameters $$\theta$$:

  $$
  \nabla_\theta\,\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[L(s,a)\big].
  $$

  **1. Score-function (REINFORCE):**

  $$
  \nabla_\theta\,\mathbb{E}_{a\sim \pi_\theta}\!\big[L(s,a)\big]
  =
  \mathbb{E}_{a\sim \pi_\theta}\!\big[L(s,a)\,\nabla_\theta \log \pi_\theta(a\mid s)\big].
  $$

  This works for any distribution, but often **high variance**.

  **2. Pathwise derivative (reparameterization):**

  If we can write $$a = g_\theta(\varepsilon, s)$$ with $$\varepsilon \sim p(\varepsilon)$$ independent of $$\theta$$, then

  $$
  \nabla_\theta \,\mathbb{E}_{\varepsilon}\!\big[\,L\!\big(s, g_\theta(\varepsilon, s)\big)\,\big]
  \;=\;
  \mathbb{E}_{\varepsilon}\!\big[\,\nabla_\theta L\!\big(s, g_\theta(\varepsilon, s)\big)\,\big].
  $$

  Autodiff can then backprop through the **deterministic** map $$g_\theta$$, yielding **much lower variance** than score-function gradients.

  In SAC with tanh-squashed Gaussians:

  $$
  \varepsilon \sim \mathcal{N}(0, I),\qquad
  z = \mu_\theta(s) + \sigma_\theta(s)\odot \varepsilon,\qquad
  a = \tanh(z).
  $$

  So $$a = g_\theta(\varepsilon, s)$$. We use the pathwise gradient for both the $$Q(s,a)$$ term and the entropy term $$\alpha \log \pi(a\mid s)$$ (the latter is well-defined thanks to the change-of-variables + Jacobian). Picture below shows the explanation from PyTorch.

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/torch_distribution.png' style="width: 100%;">
  <figcaption style="text-align: center;"> __init__.py from torch.distributions.</figcaption>
</figure>

- ### Tanh Normalization

  ### Quick clarifications

  - `log_prob` (aka $$\log p_X(x)$$) is a number for a particular sample $$x$$.

  - Entropy $$H(X)$$ is **not** $$-\log p_X(x)$$ at some $$x$$. It is the **expectation** of $$-\log p_X(X)$$ over the random variable $$X$$:

  $$
  \boxed{\,H(X)\;=\;-\;\mathbb{E}_{X}\!\left[\log p_X(X)\right]\,}.
  $$

  ```python
  import torch, math
  from torch.distributions import MultivariateNormal, TransformedDistribution, constraints
  from torch.distributions.transforms import Transform

  class Tanh2D(Transform):
      # Define the transformation properties
      domain = constraints.real_vector
      codomain = constraints.interval(-1.0, 1.0)
      bijective = True
      sign = +1
      def __init__(self): super().__init__(cache_size=1)
      def _call(self, x):  return x.tanh()
      def _inverse(self, y): return 0.5 * (y.log1p() - (-y).log1p())
      def log_abs_det_jacobian(self, x, y):
          ladj_elem = 2.0 * (math.log(2.0) - x - torch.nn.functional.softplus(-2.0 * x))
          return ladj_elem.sum(dim=-1)

  m = torch.tensor([0.5, -0.8])
  Sigma = torch.tensor([[1.4, 0.3],[0.3, 1.1]])
  X = MultivariateNormal(m, Sigma)
  Y = TransformedDistribution(X, [Tanh2D()])
  ```

  ### Change-of-variables (per sample) vs. entropy (an expectation)

  Let $$Y=f(X)$$ be a bijection with Jacobian $$J_f(x)=\tfrac{\partial y}{\partial x}$$.

  **Per-sample log-probability (what `dist.log_prob(value)` returns):**

  $$
  \boxed{\,\log p_Y(y)\;=\;\log p_X(x)\;-\;\log\!\big|\det J_f(x)\big|\,,\qquad x=f^{-1}(y)\,}.
  $$

  This is the identity used inside SAC for each sampled action.

  ### Entropy under a transform (an expectation)

  Start from the definition

  $$
  H(Y)\;=\;-\;\mathbb{E}_{Y}\!\left[\log p_Y(Y)\right].
  $$

  Change variables $$Y=f(X)$$, then plug the per-sample formula:

  $$
  \begin{aligned}
  H(Y)
  &= -\,\mathbb{E}_{X}\!\left[\log p_Y\!\big(f(X)\big)\right] \\
  &= -\,\mathbb{E}_{X}\!\left[\log p_X(X) - \log\!\big|\det J_f(X)\big|\right] \\
  &= \underbrace{-\,\mathbb{E}_{X}\!\left[\log p_X(X)\right]}_{H(X)}
  \;+\;\mathbb{E}_{X}\!\left[\log\!\big|\det J_f(X)\big|\right].
  \end{aligned}
  $$

  So

  $$
  \boxed{\,H(Y)\;=\;H(X)\;+\;\mathbb{E}_{X}\!\left[\log\!\big|\det J_f(X)\big|\right]\,}.
  $$



```python
xs = torch.tensor([[ 0.0,  0.0],
                   [ 1.2, -0.7],
                   [-1.5,  2.0]], dtype=torch.get_default_dtype())
ys = xs.tanh()
logpX = X.log_prob(xs)
logpY = Y.log_prob(ys)
logabsdet_dYdX = (2.0 * (math.log(2.0) - xs - torch.nn.functional.softplus(-2.0 * xs))).sum(dim=-1)
rhs = logpX - logabsdet_dYdX

print("=== 2D tanh transform: log-prob identity (3 points) ===")
for i in range(xs.size(0)):
    print(f"x={xs[i].tolist()}, y={ys[i].tolist()}")
    print(f"  log p_Y(y)                   = {logpY[i].item():.6f}")
    print(f"  log p_X(x) - log|det(dY/dX)| = {rhs[i].item():.6f}")
    print(f"  residual                     = {(logpY[i]-rhs[i]).item():.3e}")
    print()

HX = X.entropy().item()
N = 200_000
with torch.no_grad():
    x_samp = X.rsample((N,))
    ladj_elem = 2.0 * (math.log(2.0) - x_samp - torch.nn.functional.softplus(-2.0 * x_samp))
    E_logdetJ = ladj_elem.sum(dim=-1).mean().item()
    y_samp = torch.tanh(x_samp)
    HY_mc = -(Y.log_prob(y_samp)).mean().item()

HY_expected = HX + E_logdetJ
print("=== Entropy check for Y = tanh(X) in R^2 ===")
print(f"H(X) analytic                     = {HX:.6f}")
print(f"E_X[ log|det(dY/dX)| ] (MC)       = {E_logdetJ:.6f}")
print(f"H(Y) expected via identity        = {HY_expected:.6f}")
print(f"H(Y) Monte Carlo via samples      = {HY_mc:.6f}")

```

## Soft Actor-Critic (SAC)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/sac-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">SAC Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- **Critic Updates (with torch.no_grad and detach)**  

- **Actor Updates**  

- **Temperature Updates**  

<figure style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
  <img src='/images/blog/blog9/ant.gif' style="width: 24%;">
  <img src='/images/blog/blog9/half-cheetah.gif' style="width: 24%;">
  <img src='/images/blog/blog9/hopper.gif' style="width: 24%;">
  <img src='/images/blog/blog9/walker.gif' style="width: 24%;">
  <figcaption style="text-align: center;">SAC results on gymnasium (MuJoCo) tasks.</figcaption>
</figure>

- ### Gradient Flow (with torch.no_grad(), etc.)

## References
 1. <i class="fa-solid fa-book-open"></i> [Addressing Function Approximation Error in Actor-Critic Methodsl](https://proceedings.mlr.press/v80/fujimoto18a) (TD3 Paper)
 2. <i class="fa-solid fa-book-open"></i> [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://proceedings.mlr.press/v80/haarnoja18b) (SAC Paper)
 3. <i class="fa-solid fa-blog"></i> [TD3](https://spinningup.openai.com/en/latest/algorithms/td3.html), [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html) (OpenAI Spinning Up) 
 4. <i class="fab fa-youtube"></i> [Entropy Clearly Explained!!!](https://www.youtube.com/watch?v=YtebGVx-Fxw) & [Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM&t=17s)
 5. <i class="fa-brands fa-github"></i> [mjctrl](https://github.com/kevinzakka/mjctrl) & [RMP2](https://github.com/UWRobotLearning/rmp2)
