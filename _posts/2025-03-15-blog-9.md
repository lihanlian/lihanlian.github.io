---
title: "Implementation Details of TD3-SAC-Gymnasium"
date: 2025-12-15
permalink: /posts/blog9
tags:
  - Reinforcement Learning
  - Soft Acotr-Critic
  - Entropy
---
**Twin Delayed Deep Deterministic Policy Gradient (TD3)** and **Soft Actor-Critic (SAC)** are off-policy actor-critic algorithms designed for continuous control tasks where classic **DDPG** can be unhandy. [**TD3**](https://proceedings.mlr.press/v80/fujimoto18a) stabilizes learning with tricks such as double Q-networks, delayed policy updates, and target policy smoothing to reduce overestimation bias. [**SAC**](https://proceedings.mlr.press/v80/haarnoja18b) instead learns a stochastic policy by maximizing both task reward and **entropy**, encouraging robust and exploratory behaviors. This blog post explains core components and implementation details of both algorithms. Corresponding **PyTorch** implementation can be found at this <i class="fa-brands fa-github"></i> [repository](https://github.com/lihanlian/td3-sac-gymnasium).

## Reinforcement Learning Categories
Before diving into TD3 and SAC, it is helpful to place them within the broader landscape of reinforcement learning (RL). In this section we briefly review two common ways of categorizing RL algorithms: (i) how they represent and optimize behavior (value-based, policy-based, or actor–critic), and (ii) how they use experience (on-policy vs off-policy). This perspective will make it clearer why TD3 and SAC are usually described as **off-policy** [**actor–critic algorithms**](https://en.wikipedia.org/wiki/Actor-critic_algorithm) for continuous control.

- **Value-based vs Policy-based vs Actor–Critic**

  - **Value-based methods.**  
    Value-based algorithms learn a value function, such as a state–action value $$Q(s,a)$$ or state value $$V(s)$$, and derive a policy by acting greedily with respect to this estimate. For example, a greedy policy is  
    
    $$
      \pi(s) = \arg\max_a Q(s,a).
    $$  

    Classic examples include tabular Q-learning and deep Q-networks (DQN). The policy is *implicit* in the value function; there is usually no separate set of policy parameters.

  - **Policy-based methods (pure policy gradient).**  
    Pure policy-gradient methods parameterize the policy directly as $$\pi_\theta(a \mid s)$$ and optimize the expected return
    
    $$
      J(\theta) = \mathbb{E}_{\pi_\theta}\!\left[\sum_{t=0}^{\infty} \gamma^t r_t \right]
    $$

    via gradients of the form
    
    $$
      \nabla_\theta J(\theta)
      = \mathbb{E}_{\pi_\theta}\!\big[ \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, \hat{G}_t \big],
    $$

    where $$\hat{G}_t$$ is some return estimate. The simplest example is REINFORCE, which can work *without* a learned value function by using Monte Carlo returns as $$\hat{G}_t$$.

  - **Actor–critic methods (policy gradient with a critic).**  
    Actor–critic algorithms combine both ideas. An **actor** $$\pi_\theta(a \mid s)$$ chooses actions, while a **critic** estimates a value function (e.g., $$V_\phi(s)$$, $$Q_\phi(s,a)$$, or an advantage $$A_\phi(s,a)$$) to provide low-variance learning signals for the actor:
    
    $$
      \nabla_\theta J(\theta)
      \approx \mathbb{E}\!\big[ \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, \hat{A}_\phi(s_t,a_t) \big].
    $$

    Algorithms such as A2C/A3C, TRPO (in its common actor–critic form), PPO, TD3, and SAC fall into this category: they use a policy-gradient objective *together with* an explicit value function learned by the critic.

- **On-policy vs Off-policy**

  To explain this distinction, it is useful to separate two roles a policy can play:

  - **Behavior policy** $$\mu(a \mid s)$$: the policy that actually acts in the environment to generate data $$(s_t, a_t, r_t, s_{t+1})$$.
  - **Target policy** $$\pi(a \mid s)$$: the policy we are trying to evaluate or improve in our update rule.

  - **On-policy algorithms**  
    On-policy methods learn about a target policy using data collected from (essentially) the **same** policy:
    $$
      \mu(a \mid s) \approx \pi(a \mid s).
    $$
    In practice, the behavior policy might add a bit of exploration noise, or we might update $$\pi$$ slightly during training, but <span style="color:red"> the algorithm is designed so that the data always come from a policy very close to the one being optimized </span>.  
    Examples:
    - SARSA learns the value of the current $$\epsilon$$-greedy policy.  
    - PPO collects rollouts with an old policy $$\pi_{\text{old}}$$ and then updates $$\pi_\theta$$ while constraining it (via clipping or a KL penalty) so that $$\pi_\theta \approx \pi_{\text{old}}$$. Old data are only used for a few epochs and then discarded.

  - **Off-policy algorithms**  
    Off-policy methods allow the behavior and target policies to be **different**:
    $$
      \mu(a \mid s) \neq \pi(a \mid s).
    $$
    <span style="color:red"> This makes it possible to reuse past experience, learn from old versions of the policy, or even learn from demonstrations generated by some other agent </span>.  
    Examples:
    - Q-learning and DQN typically use an $$\epsilon$$-greedy behavior policy $$\mu$$, but update towards the greedy policy
      $$
        \pi_{\text{greedy}}(s) = \arg\max_a Q(s,a),
      $$
      so $$\mu \neq \pi_{\text{greedy}}$$.  
    - DDPG, TD3, and SAC store transitions in a replay buffer and train the current actor–critic using data that were collected many steps ago under different (more exploratory) policies. This clear separation between data collection and policy improvement is what makes them off-policy.

In the following sections we will see that TD3 and SAC sit in the intersection of these categories: they are **off-policy actor–critic** algorithms that learn a critic $$Q_\phi(s,a)$$ and a continuous-control actor $$\pi_\theta(a \mid s)$$ using replayed experience.


## Twin Delayed Deep Deterministic Policy Gradient (TD3)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/td3-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">TD3 Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Results

## Entropy


- ### Tanh Normalization

## Soft Actor-Critic (SAC)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/sac-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">SAC Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Gradient Flow (with torch.no_grad(), etc.)

## References
 1. <i class="fa-solid fa-book-open"></i> [Addressing Function Approximation Error in Actor-Critic Methodsl](https://proceedings.mlr.press/v80/fujimoto18a) (TD3 Paper)
 2. <i class="fa-solid fa-book-open"></i> [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://proceedings.mlr.press/v80/haarnoja18b) (SAC Paper)
 3. <i class="fa-solid fa-blog"></i> [TD3](https://spinningup.openai.com/en/latest/algorithms/td3.html), [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html) (OpenAI Spinning Up) 
 4. <i class="fab fa-youtube"></i> [Entropy Clearly Explained!!!](https://www.youtube.com/watch?v=YtebGVx-Fxw) & [Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM&t=17s)
 5. <i class="fa-brands fa-github"></i> [mjctrl](https://github.com/kevinzakka/mjctrl) & [RMP2](https://github.com/UWRobotLearning/rmp2)
