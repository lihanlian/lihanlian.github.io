---
title: "Implementation Details of TD3-SAC-Gymnasium"
date: 2025-12-15
permalink: /posts/blog9
tags:
  - Reinforcement Learning
  - Soft Acotr-Critic
  - Entropy
---
**Twin Delayed Deep Deterministic Policy Gradient (TD3)** and **Soft Actor-Critic (SAC)** are off-policy actor-critic algorithms designed for continuous control tasks where classic **DDPG** can be unstable. TD3 stabilizes learning with tricks such as double Q-networks, delayed policy updates, and target policy smoothing to reduce overestimation bias. SAC instead learns a stochastic policy by maximizing both task reward and **entropy**, encouraging robust and exploratory behaviors. This blog post will explain core components and go through the implementation details of both algorithms. Corresponding pytorch implementation can be found at this [repository](https://github.com/lihanlian/td3-sac-gymnasium).

## RL Categories

- ### Value-based, Policy-based, and Actor-Critic

- ### On-policy vs Off-policy


## Twin Delayed Deep Deterministic Policy Gradient (TD3)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/td3-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">TD3 Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Results

## Entropy


- ### Tanh Normalization

## Soft Actor-Critic (SAC)

<figure style="display: block; margin: 0 auto; width: 80%;">
  <img src='/images/blog/blog9/sac-openai.png' style="width: 100%;">
  <figcaption style="text-align: center;">SAC Pseudocode (OpenAI Spinning Up).</figcaption>
</figure>

- ### Gradient Flow (with torch.no_grad(), etc.)

## References
 1. <i class="fa-solid fa-book-open"></i> [Addressing Function Approximation Error in Actor-Critic Methodsl](https://proceedings.mlr.press/v80/fujimoto18a) (TD3 Paper)
 2. <i class="fa-solid fa-book-open"></i> [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://proceedings.mlr.press/v80/haarnoja18b) (SAC Paper)
 3. <i class="fa-solid fa-blog"></i> [TD3](https://spinningup.openai.com/en/latest/algorithms/td3.html), [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html) (OpenAI Spinning Up) 
 4. <i class="fab fa-youtube"></i> [Entropy Clearly Explained!!!](https://www.youtube.com/watch?v=YtebGVx-Fxw) & [Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM&t=17s)
 5. <i class="fa-brands fa-github"></i> [mjctrl](https://github.com/kevinzakka/mjctrl) & [RMP2](https://github.com/UWRobotLearning/rmp2)
